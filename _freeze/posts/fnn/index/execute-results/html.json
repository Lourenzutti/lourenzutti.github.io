{
  "hash": "fdbea785362571b634b38d21ef0827f8",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Feed Forward Neural Network\"\nauthor: \"Rodolfo Lourenzutti\"\ndate: \"2024-09-30\"\ncategories: [confidence intervals, statistical inference, analysis]\nimage: \"image.jpg\"\ndraft: true\ndraft-mode: unlinked\nformat: live-html\ncss: [../../styles/post.css]\n---\n\n\n<style>\n    main {\n        text-align: justify;    \n    }\n</style>\n<script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>\n<script src=\"https://d3js.org/d3.v6.min.js\"></script>\n<script src=\"https://polyfill.io/v3/polyfill.min.js?features=es6\"></script>\n<script id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\n<!--<script defer src=\"scripts/plot_nn.js\"></script>-->\n\n\n\n```{ojs}\n//| echo: FALSE\nfunction createNeuralNetwork(layers, div_name, drawZSquares=true, neuronRadius = 20, squareSize = 20, width = 1000, height = 400) {\n\n    const layerWidth = width / layers.length;\n    //const fontSize = neuronRadius / 2; // Set font size proportional to neuron radius\n    const fontSize = 14;\n    squareSize = drawZSquares ? squareSize : -2;\n    \n    // Clear previous content\n    //d3.select(`#${div_name}`).html('');\n\n    // Create SVG element for lines and neurons\n    const svg = d3.select(`#${div_name}`).append(\"svg\")\n        .attr(\"width\", width)\n        .attr(\"height\", height)\n        .style(\"position\", \"absolute\");\n\n    // Function to calculate Y positions\n    const calculateY = (layerIndex, nodeIndex, totalNodes) => {\n        const spacing = height / (totalNodes + 1);\n        return (nodeIndex + 1) * spacing;\n    };\n\n    // Draw connections (lines) and weight annotations\n    for (let l = 0; l < layers.length - 1; l++) {\n        for (let i = 0; i < layers[l]; i++) {\n            for (let j = 0; j < layers[l + 1]; j++) {\n                const x1 = (l + 0.5) * layerWidth;\n                const y1 = calculateY(l, i, layers[l]);\n                const x2 = (l + 1.5) * layerWidth - neuronRadius - squareSize - 2;\n                const y2 = calculateY(l + 1, j, layers[l + 1]);\n\n                // Draw line\n                svg.append(\"line\")\n                    .attr(\"x1\", x1)\n                    .attr(\"y1\", y1)\n                    .attr(\"x2\", x2)\n                    .attr(\"y2\", y2)\n                    .attr(\"stroke\", \"black\")\n                    .attr(\"stroke-width\", 1);\n\n                // Calculate position and rotation for weight annotation\n                const scaler = 0.8;\n                const annotationX = x1 + neuronRadius + (drawZSquares ? 10 : 20);\n                const slope = (y2 - y1) / (x2 - x1);\n                const angle = Math.atan(slope) * (180 / Math.PI);\n                const annotationY = y1 + slope * (annotationX - x1) - scaler*fontSize - 2;\n\n                const weightAnnotation = `w_{${j + 1}${i + 1}}^{(${l + 1})}`;\n                d3.select(`#${div_name}`).append(\"div\")\n                    .attr(\"class\", \"weight-annotation\")\n                    .style(\"left\", `${annotationX}px`)\n                    .style(\"top\", `${annotationY}px`)\n                    .style(\"font-size\", `${scaler*fontSize}px`)\n                    .style(\"transform\", `translateY(-50%) rotate(${angle}deg)`)\n                    .html(`\\\\(${weightAnnotation}\\\\)`);\n            }\n        }\n    }\n\n    // Draw neurons and neuron annotations\n    layers.forEach((numNeurons, layerIndex) => {\n        const x = (layerIndex + 0.5) * layerWidth;\n        const layerAnnotation = `Layer ${layerIndex}`;\n        svg.append(\"text\")\n            .attr(\"x\", x)\n            .attr(\"y\", 20) // Position at the top, you can adjust this value\n            .attr(\"text-anchor\", \"middle\")\n            .attr(\"font-family\", \"Arial\")\n            .attr(\"font-size\", \"16px\")\n            .text(layerAnnotation);\n        for (let i = 0; i < numNeurons; i++) {\n            const y = calculateY(layerIndex, i, numNeurons);\n\n            // Draw neuron\n            svg.append(\"circle\")\n                .attr(\"cx\", x)\n                .attr(\"cy\", y)\n                .attr(\"r\", neuronRadius)\n                .attr(\"fill\", \"steelblue\");\n\n            // Add neuron annotation\n            const neuronAnnotation = `x_{${i + 1}}^{(${layerIndex})}`;\n            d3.select(`#${div_name}`).append(\"div\")\n                .attr(\"class\", \"neuron-annotation\")\n                .style(\"left\", `${x}px`)\n                .style(\"top\", `${y}px`)\n                .style(\"font-size\", `${fontSize}px`)\n                .style('color', 'white')\n                .html(`\\\\(${neuronAnnotation}\\\\)`);\n\n            // Draw annotation for non-input neurons\n            if (layerIndex > 0 && drawZSquares) {\n                const rectX = x - neuronRadius - squareSize - 1;\n                const rectY = y - squareSize / 2;\n\n                const squareAnnotation = `z_{${i + 1}}^{(${layerIndex})}`;\n                d3.select(`#${div_name}`).append(\"div\")\n                    .attr(\"class\", \"annotation\")\n                    .style(\"left\", `${rectX}px`)\n                    .style(\"top\", `${rectY}px`)\n                    .style(\"width\", `${squareSize}px`)\n                    .style(\"height\", `${squareSize}px`)\n                    .style(\"line-height\", `${squareSize}px`)\n                    .style(\"font-size\", `${0.8*fontSize}px`)\n                    .html(`\\\\(${squareAnnotation}\\\\)`)\n                    .on(\"click\", () => propagateFromZSquare(layerIndex, i));;\n            }\n        }\n    });\n    \n    // Function to animate impulse\n    function animateImpulse(startX, startY, endX, endY, duration) {\n        const impulse = svg.append(\"circle\")\n            .attr(\"cx\", startX)\n            .attr(\"cy\", startY)\n            .attr(\"r\", 5)\n            .attr(\"fill\", \"red\");\n\n        impulse.transition()\n            .duration(duration)\n            .attr(\"cx\", endX)\n            .attr(\"cy\", endY)\n            .on(\"end\", () => impulse.remove());\n    }\n\n    // Function to propagate impulse\n    function propagateImpulse(layerIndex, neuronIndex) {\n        if (layerIndex < layers.length - 1) {\n            for (let j = 0; j < layers[layerIndex + 1]; j++) {\n                const startX = (layerIndex + 0.5) * layerWidth;\n                const startY = calculateY(layerIndex, neuronIndex, layers[layerIndex]);\n                const endX = (layerIndex + 1.5) * layerWidth - neuronRadius - squareSize - 2;\n                const endY = calculateY(layerIndex + 1, j, layers[layerIndex + 1]);\n\n                animateImpulse(startX, startY, endX, endY, 1200);\n\n                // Recursive call for next layer\n                setTimeout(() => propagateImpulse(layerIndex + 1, j), 1000);\n            }\n        }\n    }\n\n    // Function to propagate impulse from a Z square\n    function propagateFromZSquare(layerIndex, neuronIndex) {\n        if (layerIndex < layers.length - 1) {\n            for (let j = 0; j < layers[layerIndex + 1]; j++) {\n                const startX = (layerIndex + 0.5) * layerWidth;\n                const startY = calculateY(layerIndex, neuronIndex, layers[layerIndex]) + squareSize / 2;\n                const endX = (layerIndex + 1.5) * layerWidth - neuronRadius - squareSize - 2;\n                const endY = calculateY(layerIndex + 1, j, layers[layerIndex + 1]);\n\n                animateImpulse(startX, startY, endX, endY, 1200);\n            }\n        }\n    }\n\n    // Draw neurons and neuron annotations\n    layers.forEach((numNeurons, layerIndex) => {\n        for (let i = 0; i < numNeurons; i++) {\n            const x = (layerIndex + 0.5) * layerWidth;\n            const y = calculateY(layerIndex, i, numNeurons);\n\n            // Draw neuron\n            svg.append(\"circle\")\n                .attr(\"cx\", x)\n                .attr(\"cy\", y)\n                .attr(\"r\", neuronRadius)\n                .attr(\"fill\", \"steelblue\")\n                .attr(\"cursor\", \"pointer\")\n                .on(\"click\", () => propagateImpulse(layerIndex, i));\n\n        }\n    });\n    // Render MathJax\n    MathJax.typesetPromise();\n}\n```\n\n\n## Learning Objectives and Pre-Requisites\n\nIn this tutorial, we will explore how feedforward neural networks work. We'll discuss neurons, layers, activation functions, and cost functions. Then, we'll see in detail how we train a neural network using backpropagation. We will derive the backpropagation formulae step-by-step and implement a neural network from scratch using Python's Numpy package only. \n\n**Learning Objectives:** <br>\n\nAt the end of this tutorial, the reader should be able to:\n\n- Calculate the forward pass of a neural network;\n- Calculate the backpropagation of a neural network;\n- Implement a neural network from scratch in Python;\n- Implement a neural network using PyTorch;\n\n**Prerequisites:** <br>\n\nIt is assumed that the reader:\n\n- is proficient in computing derivatives, particularly in the application of the *Chain Rule*.\n- has some Python knowledge;\n- is able to perform matrix multiplication;\n \n## Introduction\n\nNeural Networks are certainly among the most \"famous\" models in machine learning. They power many tools that we use nowadays, from computer vision to generative models and medical applications. If this is not the first post you've read about neural networks, you surely have seen a diagram like the one below.\n\n:::{#fig-nn-regular}\n\n<img src=\"images/nn1.svg\" width=\"100%\">\n\nThe usual Neural Network diagram (bias neurons omitted).\n:::\n\nWhen I started learning about neural networks, I found the standard diagram confusing because it doesn't explicitly show a crucial component that will be needed later for the backpropagation algorithm. Therefore, for this tutorial, we will explicitly include this component in the diagram, as shown in @fig-nn-diagram. \n\n:::{#fig-nn-diagram}\n\n<img src=\"images/nn2.svg\" width=\"100%\">\n\nA Neural Network diagram including the linear aggregator $z^{(l)}_{i}$  (again, bias neurons omitted).\n:::\n\nLet's introduce some terminology:\n\n- **Layers**: This neuron network has four layers. \n    - **Input layer:** the first layer is known as the input layer; it brings the data into the network.\n    - **Output layer:** the last layer is known as the output layer; it provides the numerical outputs of the neural network.\n    - **Hidden layers:** the layers between the input and output layers are known as the hidden layers; in this case, layers 1 and 2 are hidden layers. \n\n- **Neurons:** the blue circles are the so-called neurons; neurons send a numerical value as a signal for the neurons in the following layer. \n  - Different layers can have different numbers of neurons.\n  - The signals neurons in the input layer send are the data. \n  - The number of neurons in the input layer is the number of attributes in the dataset.\n  - **Activation function:** a non-linear function that specifies how neurons process the signals they receive. This function is not explicitly showed in the graph, but it is \"inside\" the neuron. \n\n- **Weights:** the weights are numerical values (positive or negative) that amplify or reduce the strength of a neuron's signal to another neuron; they are represented in the graph by the lines;\n\n- **Receptors:** we will call the boxes attached to each neuron the neuron's receptor, which will collect and aggregate all the signals a neuron receives from other neurons (this is not standard language); \n\nI've always found the terminology very confusing without looking at the equations. For example, when I say that weights amplify or reduce the signal, how exactly does that happen? How exactly do receptors collect and aggregate all the signals? How do neurons process the signals passed by the receptors? Before we go over these in detail, let's review the notation we are using.\n\n## Notation\n\n:::{#fig-nn-notation}\n\n<img src=\"images/nn3.jpg\" width=\"100%\">\n\nThe weight $w_{1,2}^{(1)}$ connects the neurons $X^{(0)}_1$ and $X^{(1)}_2$.\n:::\n\n- $(l)$ refers to the layer, and goes from 0 to $L$, where the $L$th layer is the output layer. \n- $n^{(l)}$ is the number of neurons in layer $l$.\n- $x^{(l)}_{k}$ is the $k$th neuron in layer $l$.\n  - $x^{(l)}_{i,k}$ is the value of the $k$th neuron in layer $l$ for the $i$th training sample.\n- $z^{(l)}_{k}$ is the receptor of neuron $x^{(l)}_k$.\n  - $z^{(l)}_{i,k}$ is the value of the receptor of neuron $x^{(l)}_k$ for the $i$th training sample.\n- $w_{i,j}^{(l)}$ is the weight connecting the $i$th neuron in layer $l-1$ to the $j$th neuron in layer $l$.\n- $b^{(l)}_k$ the bias term added by the receptor of neuron $k$ in layer $l$.\n\nSince we have a ton of weights, it is helpful for us to organize them into matrices. We will have one weight matrix per layer (except for layer 0). We will denote the matrices as ${\\bf{W}}^{(l)}$. @fig-nn-matrix-weights illustrates how the weights are organized into matrices for our example neural network.\n\n:::{#fig-nn-matrix-weights}\n\n<img src=\"images/nn4.svg\" width=\"100%\">\n\nThe weight $w_{1,2}^{(1)}$ connects the neurons $X^{(0)}_1$ and $X^{(1)}_2$.\n:::\n\nThe matrix ${\\bf{W}}^{(l)}$ contains the weights connecting neurons in layer $l-1$ to neurons in layer $l$. It has $n^{(l-1)}$ rows and $n^{(l)}$ columns. The $i$th row of ${\\bf{W}}^{(l)}$ are all weights \"leaving\" neuron $i$ from layer $l-1$. The $j$th column of ${\\bf{W}}^{(l)}$ are all the weights \"arriving\" in neuron $j$ in layer $l-1$. @fig-weight-matrix-shape illustrates these points. \n\n:::{#fig-weight-matrix-shape}\n\n<img src=\"images/nn5.jpg\" width=\"100%\">\n\nShape of matrix ${\\bf{W}}^{(1)}$.\n:::\n\nFor example, the second row of ${\\bf{W}}^{(2)}$ has all the weights leaving neuron 2 from layer 1, as shown in @fig-weight-matrix-row; while the second column has all the weights arriving at neuron 2 in layer 2, as illustrated in @fig-weight-matrix-column.\n\n:::{#fig-weight-matrix-row}\n\n<img src=\"images/nn6.svg\" width=\"100%\">\n\nThe second row of ${\\bf{W}}^{(2)}$ has all weights \"leaving\" neuron 2 in layer 1.\n:::\n\n\n:::{#fig-weight-matrix-column}\n\n<img src=\"images/nn7.svg\" width=\"100%\">\n\nThe second column of ${\\bf{W}}^{(2)}$ has all weights \"arriving\" at neuron 2 in layer 2.\n:::\n\n\nNow that we understand the notation, we are ready to introduce the necessary equations.\n\n\n- **Receptors**: The value of the $k$th  receptor in layer $l$ for the $i$th training sample is given by: $$z^{(l)}_{i, k} =\\sum_{j=1}^{n^{(l-1)}} w^{(l)}_{j, k}x^{(l-1)}_{i,j} + b^{(l)}_k,\\quad l=1,...,L, \\quad \\text{and} \\quad j=1,...,n^{(l)}$$\n- **Neurons** (for the $i$th training sample): $$x^{(l)}_{i, k}=a\\left(z^{(l)}_{i, k}\\right),\\quad l=1,...,L, \\quad \\text{and} \\quad j=1,...,n^{(l)}$$\nwhere $a$ is a non-linear function called **activation function**. We will discuss activation functions in more detail later. For now, we will use $a(x)=\\max\\left\\{0, x\\right\\}$.\n  - Note: for the input layer, $l=0$, $x^{(0)}_j$ is just the feature $j$ of the input vector. \n\nOkay, I agree; the notation is heavy. We have a lot of things to keep track of, such as layers, receptors, neurons, and weights, so we need a lot of symbols and indices. For this reason, I encourage the reader to go back to @fig-nn-diagram, pick a neuron in a hidden layer, and write down the equations for that neuron while identifying the elements being used in the diagram.\n\n:::{#exm-forward-pass}\n\nFor us to go through an example of the feedforward part of the neural network, let us get some synthetic data with two features as well as define some values for the weights.     \n\n:::{#fig-synthetic-data}\n\n<img src=\"images/nn8.jpg\" width=\"80%\">\n\nSome synthetic data and weights for our neural network.\n:::\n\nNow, we can calculate the forward pass of the neural network. Let's do it for the first row in our data, i.e., for the input vector $x=(6.1, 8.7)$.\n\n:::: {.columns}\n\n::: {.column width=\"45%\"}\n\n- Receptors in Layer 1:\n$$z^{(1)}_1 =\\sum_{i=1}^{2} w^{(1)}_{i, 1}x^{(0)}_i = -0.314$$\n$$z^{(1)}_2 =\\sum_{i=1}^{2} w^{(1)}_{i, 2}x^{(0)}_i =  0.818$$\n$$z^{(1)}_3 =\\sum_{i=1}^{2} w^{(1)}_{i, 3}x^{(0)}_i = -0.417$$\n\n- Receptors in Layer 2:\n$$z^{(2)}_1 =\\sum_{i=1}^{3} w^{(2)}_{i, 1}x^{(1)}_i = -0.07362$$\n$$z^{(2)}_2 =\\sum_{i=1}^{3} w^{(2)}_{i, 2}x^{(1)}_i =  0.02454$$\n\n- Receptor in Layer 3:\n$$z^{(3)}_1 =\\sum_{i=1}^{2} w^{(3)}_{i, 1}x^{(1)}_i = 0.0017178$$\n\n:::\n\n::: {.column width=\"5%\"}\n<!-- empty column to create gap -->\n:::\n\n::: {.column width=\"45%\"}\n\n- Neurons in Layer 1:\n$$x^{(1)}_1 = \\max\\left\\{0, z^{(1)}_1\\right\\}  = 0\\ \\ \\ \\ \\ \\ \\textcolor{white}{\\sum_{i=1}^{2}}$$\n$$x^{(1)}_2 = \\max\\left\\{0, z^{(1)}_2\\right\\} = 0.818 \\textcolor{white}{\\sum_{i=1}^{2}}$$\n$$x^{(1)}_3 = \\max\\left\\{0, z^{(1)}_3\\right\\} = 0\\ \\ \\ \\ \\ \\ \\textcolor{white}{\\sum_{i=1}^{2}}$$\n\n- Neurons in Layer 2:\n$$x^{(2)}_1 = \\max\\left\\{0, z^{(2)}_1\\right\\} = 0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\textcolor{white}{\\sum_{i=1}^{2}}$$\n$$x^{(2)}_2 = \\max\\left\\{0, z^{(2)}_2\\right\\} = 0.02454 \\textcolor{white}{\\sum_{i=1}^{2}}$$\n\n- Neuron in Layer 3:\n$$x^{(3)}_1 = z^{(3)}_1 = 0.0017178$$\n\n:::\n\n::::\n\nLet's now visualize this result in the diagram. We will use two decimal places due to space constraints.\n\n:::{#fig-forward-pass}\n\n<img src=\"images/nn9.svg\" width=\"100%\">\n\nForward pass of the neural network for the input vector $x=(6.1, 8.7)$.\n:::\n\n:::\n\nCongratulations! You have completed the forward pass of the neural network. \n\nI'll just note here that the activation function used in the output layer usually changes according to the problem. For example, for regression problems, a common choice is the identity function $f(z) = z$. If we used $f(z) = \\max\\left\\{0, z\\right\\}$, we would never be able to predict a negative value, an undesirable property. \n\n### Matrix Notation\n\nTo implement a Neural Network in Python from scratch, we will need to use the highly optimized Numpy's vectorization; so, let's introduce the matrix notation here. The matrix notation also has the advantage of simplifying the steps. Note that everything is almost exactly the same; the only difference is that, with the matrix notation, we will be considering the entire dataset. \n\nWe will denote matrices with capital bold letters (e.g., $\\bf{X}$, ${\\bf{W}}^{(1)}$), vectors as lowercase bold letters (e.g., ${\\bf{x}}_1$). Also, vectors are always column vectors (multiple rows, one column). Here are the equations in matrix format:\n\n- Receptors:\n$$\n{\\bf{Z}}^{(l)} = {\\mathbf{X}^{(l-1)}} \\mathbf{W}^{(l)} + \\mathbf{1}_n\\left(\\mathbf{b}^{(l)}\\right)^T\n$$ {#eq-matrix-receptor}\nwhere $\\mathbf{1}_n$ is a column vector of ones with $n$ rows and $T$ stands for transpose. The operation $\\mathbf{1}\\left(\\mathbf{b}^{(l)}\\right)^T$ is Numpy's broadcast.\n\n\n- Neurons:\n$$\n{\\bf{X}}^{(l)} = a\\left({\\bf{Z}}^{(l)}\\right) \n$$ {#eq-matrix-neuron}\nwhere $a\\left({\\bf{Z}}^{(l)}\\right)$ means we apply the activation function $a$ to every single element of ${\\bf{Z}}^{(l)}$.\n\nThe notation becomes much simpler, doesn't it? Let's take a closer look at ${\\bf{Z}}^{(l)}$ in @fig-matrix-receptor .\n\n:::{#fig-matrix-receptor}\n\n<img src=\"images/nn10.jpg\" width=\"100%\">\n\nThe representation of ${\\bf{Z}}^{(l)}$.\n:::\n\nNow, let's go through @exm-forward-pass again, but this time, we will use matrix notation. \n\n:::{#exm-forward-pass-matrix} \n\nWe will use the same data and weights as before, as shown in @fig-synthetic-data. The forward pass involves calculating the values of the receptors and neurons for each layer of the network and each input vector. We will start by calculating ${\\bf{Z}}^{(1)}$, ${\\bf{X}}^{(1)}$, ${\\bf{Z}}^{(2)}$, ${\\bf{X}}^{(2)}$, ${\\bf{Z}}^{(3)}$, and ${\\bf{X}}^{(3)}$ for all input vectors. We will first do it mathematically, then we will implement it in Python.\n\n- We start by calculating ${\\bf{Z}}^{(1)}$ and ${\\bf{X}}^{(1)}$:\n\n:::{#fig-matrix-example-layer1}\n\n<img src=\"images/nn11.svg\" width=\"100%\">\n\nValues of the receptors and neurons in the first layer for all input vectors.\n:::\n\n- Using the results from the first layer, we can calculate ${\\bf{Z}}^{(2)}$ and ${\\bf{X}}^{(2)}$:\n\n:::{#fig-matrix-example-layer2}\n\n<img src=\"images/nn12.svg\" width=\"100%\">\n\nValues of the receptors and neurons in the second layer for all input vectors.\n:::\n\n- Using the results from the second layer, we can calculate ${\\bf{Z}}^{(3)}$ and ${\\bf{X}}^{(3)}$:\n\n:::{#fig-matrix-example-layer3}\n\n<img src=\"images/nn13.svg\" width=\"85%\">\n\nValues of the receptors and neurons in the third layer for all input vectors.\n:::\n\n:::\n\nNow that we have the results for all layers and all input vectors, we can implement the forward pass of the neural network in Python. We start by generating the data and initializing the weights.\n\n\n\n```{pyodide}\n# We need the numpy library\nimport numpy as np\n\n# Creating a random number generator (seed=1 for reproducibility)\nrng = np.random.default_rng(seed=1)\n\n# Creating the features data \nX = np.round(rng.uniform(3, 9, size=(10,2)), 1)\n\n# Creating response data\ny = np.round(np.sum(X**2, axis=1) + rng.normal(loc=0, scale = 3, size=10))\n\n# Initializing weights\nW1 = np.array([[-0.08, 0.02, 0.06], [0.02, 0.08, -0.09]])\nW2 = np.array([[0.01, -0.01], [-0.09, 0.03], [0.07, 0.02]])\nW3 = np.array([[-0.05], [0.07]])\n\n# Printing the data and weights\nprint(f'X: {X}\\n\\nY: {Y}\\n\\nW1: {W1}\\n\\nW2: {W2}\\n\\nW3: {W3}')\n```\n\n\nGreat, this matches the data and weights in @fig-synthetic-data.\n\nNext, we can calculate ${\\bf{Z}}^{(1)}$, ${\\bf{Z}}^{(2)}$, and ${\\bf{Z}}^{(3)}$ and ${\\bf{X}}^{(1)}$, ${\\bf{X}}^{(2)}$, and ${\\bf{X}}^{(3)}$ \n\n\n```{pyodide}\nZ1 = X @ W1\nX1 = np.fmax(0, Z1)\n\nZ2 = X1 @ W2\nX2 = np.fmax(0, Z2)\n\nZ3 = X2 @ W3\nX3 = np.fmax(0, Z3)\n\n# Printing the results\nprint(f'Z1: {Z1}\\n\\nX1: {X1}\\n\\nZ2: {Z2}\\n\\nX2: {X2}\\n\\nZ3: {Z3}\\n\\nX3: {X3}')\n```\n\n\nWe successfully calculated the values of the receptors and neurons for all layers and input vectors using NumPy's vectorization.\n\nBy now, you should have a clear understanding of how the forward pass operates. If not, do not move to the next section. Practice a little more with the information above. There is a lot going on; the notation is heavy, and the matrix notation is a bit confusing at first. Once you are comfortable with the forward pass, we can move on to the backpropagation algorithm. Backpropagation is the algorithm we use to train the neural network. Without training, the output of the neural network is meaningless.\n\n## Backpropagation\n\nBackpropagation is a critical algorithm employed in the training of neural networks. Its foundation lies in the chain rule of calculus, which allows us to compute how changes in the weights affect the overall performance of the network. The algorithm  is named for its distinctive process of error propagation. \nThe algorithm transmits the error from the output layer back through the various layers of the network all the way to the input layer. During this backward pass, the algorithm essentially assesses how much each weight contributed to the overall error, allowing for precise adjustments to be made. By updating these weights using the calculated gradients, the network can gradually improve its predictions and performance. \n\n### Measuring the error: the cost function\n\nBefore we delve into the backpropagation algorithm, we need to define a cost function. The cost function quantifies how inaccurate the network's predictions are. The goal of the training process is to minimize this cost function.\n\nFor example, for regression, a commonly used cost function is:\n\n$$\nJ(\\mathbf{w}|\\mathbf{X}, \\mathbf{y}) = \\frac{1}{2n}\\sum_{i=1}^{n} \\left(\\hat{y}_i - y_i\\right)^2\n$$ {#eq-cost-mse}\n   \nwhere $\\hat{y}_i$ is the predicted value for the $i$th sample, $y_i$ is the true value, and $n$ is the number of samples. Note that the cost function depends on the weights $\\mathbf{w}$, the input data $\\mathbf{X}$, and the true values $\\mathbf{y}$. But the input data and true values are fixed (it is the data we have); we can only change the weights. Classification problems have different cost functions, such as the cross-entropy loss (let's not worry about this for now).\n\n### The backpropagation algorithm\n\nThe entire idea of backpropagation is to calculate the gradient of the cost function with respect to the weights. To do this, we need to pass through the elements of the network in reverse order. We will start by calculating the gradient of the cost function with respect to the neurons in the output layer. Then, we will calculate the gradient of the cost function with respect to the receptors in the output layer. Finally, we will calculate the gradient of the cost function with respect to the weights.\n\nTo visualize the backpropagation step-by-step, let us know so we can update our neural network diagram with the cost function, as shown in @fig-nn-diagram-cost.\n\n:::{#fig-nn-diagram-cost}\n\n<img src=\"images/nn14.jpg\" width=\"100%\">\n\nNeural Network diagram with cost function.\n::: \n\n\nFor now, let's do this for a single input vector (as if our data matrix $X$ had only one row). Remember, the weights are the only thing we can change; the data and true values are fixed. So, we want to calculate the gradient of the cost function with respect to the weights. Let's start with the output layer and calculate the derivative of $w_{1,1}^{(3)}$. We will calculate this derivative using the chain rule:\n\n1. First, we calculate the derivative of the cost function with respect to the neuron $x^{(3)}_1$: $\\frac{\\partial J}{\\partial x^{(3)}_1}$. \n\n\n2. Then, we calculate the derivative of the neuron $x^{(3)}_1$ with respect to the receptor $z^{(3)}_1$: $\\frac{\\partial x^{(3)}_1}{\\partial z^{(3)}_1}$.\n\n\n3. Finally, we calculate the derivative of the receptor $z^{(3)}_1$ with respect to the weight $w_{1,1}^{(3)}$: $\\frac{\\partial z^{(3)}_1}{\\partial w_{1,1}^{(3)}}$\n\nThen, we combine these using the chain rule:\n$$\n\\frac{\\partial J}{\\partial w_{1,1}^{(3)}} = \\frac{\\partial J}{\\partial x^{(3)}_1}\\frac{\\partial x^{(3)}_1}{\\partial z^{(3)}_1}\\frac{\\partial z^{(3)}_1}{\\partial w_{1,1}^{(3)}}\n$$ \n\n@fig-backprop-w1-1-3 illustrates the calculation of the derivative of $w_{1,1}^{(3)}$.\n\n:::{#fig-backprop-w1-1-3}\n\n<img src=\"images/nn15.jpg\" width=\"100%\">\n\nChain rule for the derivative of $w_{1,1}^{(3)}$.\n::: \n\nAll that is left for us to do now is to calculate the derivatives. To be able to do this, we need to specify the cost and activation functions. We will use the mean squared error cost function and the ReLU activation function we have used so far.\n\n$$\n\\frac{\\partial J}{\\partial x^{(3)}_1} = \\frac{\\partial}{\\partial x^{(3)}_1}\\left[\\frac{1}{2}\\left(x^{(3)}_1 - y_1\\right)^2\\right] = x^{(3)}_1 - y_1\n$$\n\nNext, remember we are using the identity function as the activation function for the last layer since this is a regression problem, i.e., $x^{(3)}_1=z^{(3)}_1$.\n$$\n\\frac{\\partial x^{(3)}_1}{\\partial z^{(3)}_1} = \\frac{\\partial}{\\partial z^{(3)}_1}z^{(3)}_1 = 1\n$$\n\nFinally,\n$$\n\\frac{\\partial z^{(3)}_1}{\\partial w_{1,1}^{(3)}} = \\frac{\\partial}{\\partial w_{1,1}^{(3)}}\\left[\\sum_{i=1}^{2} w^{(3)}_{i, 1}x^{(2)}_i\\right] = x^{(2)}_1\n$$\n\nNote the presence of $x^{(2)}_1$ and $x^{(3)}_1$ in the derivatives. These are the values obtained in the forward pass. The diagram @fig-backprop-w1-1-3-values illustrates the values of the derivatives for the example we have been working on.\n\n:::{#fig-backprop-w1-1-3-values}\n\n<img src=\"images/nn16.jpg\" width=\"100%\">\n\nValues of the derivatives for the calculation of $\\frac{\\partial J}{\\partial w_{1,1}^{(3)}}$. (The values here are rounded. For more precise values check the first row of `Z3`, `X3`, and `X2` in the python code above.)\n::: \n\nTherefore, the derivative of $w_{1,1}^{(3)}$ is:\n$$\n\\frac{\\partial J}{\\partial w_{1,1}^{(3)}} = (x^{(3)}_1 - y_1) \\times 1 \\times x^{(2)}_1 = (0.0017178 - 113) \\times 1 \\times 0 = 0\n$$\n\nSimilarly, we can calculate the derivative of $w_{2,1}^{(3)}$:\n\n:::{#fig-backprop-w2-1-3}\n\n<img src=\"images/nn17.jpg\" width=\"100%\">\n\nChain rule for the derivative of $w_{2,1}^{(3)}$.\n::: \n\n$$\n\\frac{\\partial J}{\\partial w_{2,1}^{(3)}} = (0.0017178 - 113) \\times 1 \\times 0.2 = -22.6\n$$\n\nHopefully, at this point, you are convinced that we can calculate the derivatives of the output layer weights. Now, let's see how to calculate the derivative of a weight in the hidden layers. We will start by calculating the derivative of $w_{3,2}^{(2)}$. @fig-backprop-w3-2-2 illustrates the calculation of the derivative of $w_{3,2}^{(2)}$.\n\n:::{#fig-backprop-w3-2-2}\n\n<img src=\"images/nn18.jpg\" width=\"100%\">\n\nChain rule for the derivative of $w_{3,2}^{(2)}$.\n::: \n\nThe key aspect here is for you to see that we just \"expand\" the path (note the blue path in @fig-backprop-w3-2-2). The red part of the path has already been calculated in the previous step, which is the derivative of $w_{2,1}^{(3)}$. Therefore, we just need to multiply the partial derivatives in the blue part of the path to get the derivative of $w_{3,2}^{(2)}$.\n\n$$\n\\frac{\\partial J}{\\partial w_{3,2}^{(2)}} = \\frac{\\partial J}{\\partial x^{(3)}_1}\\frac{\\partial x^{(3)}_1}{\\partial z^{(3)}_1}\\frac{\\partial z^{(3)}_1}{\\partial x^{(2)}_1}\\frac{\\partial x^{(2)}_1}{\\partial z^{(2)}_2}\\frac{\\partial z^{(2)}_2}{\\partial w_{3,2}^{(2)}}\n$$\n\n<!-- $$\n\\frac{\\partial x^{(3)}_1}{\\partial z^{(3)}_1} = \\frac{\\partial}{\\partial z^{(3)}_1}\\left[\\max\\left\\{0, z^{(3)}_1\\right\\}\\right] = \\begin{cases} 1, & \\text{if } z^{(3)}_1 > 0 \\\\ 0, & \\text{otherwise} \\end{cases}\n$$ -->\n\n\n## Implementation \n\n### From scratch with Numpy\n\n### Using Pytorch\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}