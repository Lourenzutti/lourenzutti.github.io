<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Rodolfo Lourenzutti">
<meta name="dcterms.date" content="2024-09-30">

<title>Feed Forward Neural Network â€“ Rodolfo Lourenzutti</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-d4be639c637f3db3c684c66cefad7e0c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../../site_libs/quarto-contrib/live-runtime/live-runtime.js" type="module"></script>
<link href="../../site_libs/quarto-contrib/live-runtime/live-runtime.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script type="module" src="../../site_libs/quarto-ojs/quarto-ojs-runtime.js"></script>
<link href="../../site_libs/quarto-ojs/quarto-ojs.css" rel="stylesheet">
<meta name="quarto:status" content="draft">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles/post.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner"><div id="quarto-draft-alert" class="alert alert-warning"><i class="bi bi-pencil-square"></i>Draft</div>
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Rodolfo Lourenzutti</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Feed Forward Neural Network</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">confidence intervals</div>
                <div class="quarto-category">statistical inference</div>
                <div class="quarto-category">analysis</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Rodolfo Lourenzutti </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 30, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<style>
    main {
        text-align: justify;    
    }
</style>
<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
<script src="https://d3js.org/d3.v6.min.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!--<script defer src="scripts/plot_nn.js"></script>-->
<div class="cell">
<div class="sourceCode cell-code hidden" id="cb1" data-startfrom="26" data-source-offset="0"><pre class="sourceCode js code-with-copy"><code class="sourceCode javascript" style="counter-reset: source-line 25;"><span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">createNeuralNetwork</span>(layers<span class="op">,</span> div_name<span class="op">,</span> drawZSquares<span class="op">=</span><span class="kw">true</span><span class="op">,</span> neuronRadius <span class="op">=</span> <span class="dv">20</span><span class="op">,</span> squareSize <span class="op">=</span> <span class="dv">20</span><span class="op">,</span> width <span class="op">=</span> <span class="dv">1000</span><span class="op">,</span> height <span class="op">=</span> <span class="dv">400</span>) {</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    <span class="kw">const</span> layerWidth <span class="op">=</span> width <span class="op">/</span> layers<span class="op">.</span><span class="at">length</span><span class="op">;</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    <span class="co">//const fontSize = neuronRadius / 2; // Set font size proportional to neuron radius</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    <span class="kw">const</span> fontSize <span class="op">=</span> <span class="dv">14</span><span class="op">;</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    squareSize <span class="op">=</span> drawZSquares <span class="op">?</span> squareSize <span class="op">:</span> <span class="op">-</span><span class="dv">2</span><span class="op">;</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Clear previous content</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    <span class="co">//d3.select(`#${div_name}`).html('');</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Create SVG element for lines and neurons</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    <span class="kw">const</span> svg <span class="op">=</span> d3<span class="op">.</span><span class="fu">select</span>(<span class="vs">`#</span><span class="sc">${</span>div_name<span class="sc">}</span><span class="vs">`</span>)<span class="op">.</span><span class="fu">append</span>(<span class="st">"svg"</span>)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>        <span class="op">.</span><span class="fu">attr</span>(<span class="st">"width"</span><span class="op">,</span> width)</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        <span class="op">.</span><span class="fu">attr</span>(<span class="st">"height"</span><span class="op">,</span> height)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        <span class="op">.</span><span class="fu">style</span>(<span class="st">"position"</span><span class="op">,</span> <span class="st">"absolute"</span>)<span class="op">;</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Function to calculate Y positions</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    <span class="kw">const</span> calculateY <span class="op">=</span> (layerIndex<span class="op">,</span> nodeIndex<span class="op">,</span> totalNodes) <span class="kw">=&gt;</span> {</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>        <span class="kw">const</span> spacing <span class="op">=</span> height <span class="op">/</span> (totalNodes <span class="op">+</span> <span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (nodeIndex <span class="op">+</span> <span class="dv">1</span>) <span class="op">*</span> spacing<span class="op">;</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    }<span class="op">;</span></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Draw connections (lines) and weight annotations</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (<span class="kw">let</span> l <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> l <span class="op">&lt;</span> layers<span class="op">.</span><span class="at">length</span> <span class="op">-</span> <span class="dv">1</span><span class="op">;</span> l<span class="op">++</span>) {</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="kw">let</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> layers[l]<span class="op">;</span> i<span class="op">++</span>) {</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> (<span class="kw">let</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> layers[l <span class="op">+</span> <span class="dv">1</span>]<span class="op">;</span> j<span class="op">++</span>) {</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>                <span class="kw">const</span> x1 <span class="op">=</span> (l <span class="op">+</span> <span class="fl">0.5</span>) <span class="op">*</span> layerWidth<span class="op">;</span></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>                <span class="kw">const</span> y1 <span class="op">=</span> <span class="fu">calculateY</span>(l<span class="op">,</span> i<span class="op">,</span> layers[l])<span class="op">;</span></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>                <span class="kw">const</span> x2 <span class="op">=</span> (l <span class="op">+</span> <span class="fl">1.5</span>) <span class="op">*</span> layerWidth <span class="op">-</span> neuronRadius <span class="op">-</span> squareSize <span class="op">-</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>                <span class="kw">const</span> y2 <span class="op">=</span> <span class="fu">calculateY</span>(l <span class="op">+</span> <span class="dv">1</span><span class="op">,</span> j<span class="op">,</span> layers[l <span class="op">+</span> <span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>                <span class="co">// Draw line</span></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>                svg<span class="op">.</span><span class="fu">append</span>(<span class="st">"line"</span>)</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>                    <span class="op">.</span><span class="fu">attr</span>(<span class="st">"x1"</span><span class="op">,</span> x1)</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>                    <span class="op">.</span><span class="fu">attr</span>(<span class="st">"y1"</span><span class="op">,</span> y1)</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>                    <span class="op">.</span><span class="fu">attr</span>(<span class="st">"x2"</span><span class="op">,</span> x2)</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>                    <span class="op">.</span><span class="fu">attr</span>(<span class="st">"y2"</span><span class="op">,</span> y2)</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>                    <span class="op">.</span><span class="fu">attr</span>(<span class="st">"stroke"</span><span class="op">,</span> <span class="st">"black"</span>)</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>                    <span class="op">.</span><span class="fu">attr</span>(<span class="st">"stroke-width"</span><span class="op">,</span> <span class="dv">1</span>)<span class="op">;</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>                <span class="co">// Calculate position and rotation for weight annotation</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>                <span class="kw">const</span> scaler <span class="op">=</span> <span class="fl">0.8</span><span class="op">;</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>                <span class="kw">const</span> annotationX <span class="op">=</span> x1 <span class="op">+</span> neuronRadius <span class="op">+</span> (drawZSquares <span class="op">?</span> <span class="dv">10</span> <span class="op">:</span> <span class="dv">20</span>)<span class="op">;</span></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>                <span class="kw">const</span> slope <span class="op">=</span> (y2 <span class="op">-</span> y1) <span class="op">/</span> (x2 <span class="op">-</span> x1)<span class="op">;</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>                <span class="kw">const</span> angle <span class="op">=</span> <span class="bu">Math</span><span class="op">.</span><span class="fu">atan</span>(slope) <span class="op">*</span> (<span class="dv">180</span> <span class="op">/</span> <span class="bu">Math</span><span class="op">.</span><span class="cn">PI</span>)<span class="op">;</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>                <span class="kw">const</span> annotationY <span class="op">=</span> y1 <span class="op">+</span> slope <span class="op">*</span> (annotationX <span class="op">-</span> x1) <span class="op">-</span> scaler<span class="op">*</span>fontSize <span class="op">-</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>                <span class="kw">const</span> weightAnnotation <span class="op">=</span> <span class="vs">`w_{</span><span class="sc">${</span>j <span class="op">+</span> <span class="dv">1</span><span class="sc">}${</span>i <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="vs">}^{(</span><span class="sc">${</span>l <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="vs">)}`</span><span class="op">;</span></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>                d3<span class="op">.</span><span class="fu">select</span>(<span class="vs">`#</span><span class="sc">${</span>div_name<span class="sc">}</span><span class="vs">`</span>)<span class="op">.</span><span class="fu">append</span>(<span class="st">"div"</span>)</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>                    <span class="op">.</span><span class="fu">attr</span>(<span class="st">"class"</span><span class="op">,</span> <span class="st">"weight-annotation"</span>)</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>                    <span class="op">.</span><span class="fu">style</span>(<span class="st">"left"</span><span class="op">,</span> <span class="vs">`</span><span class="sc">${</span>annotationX<span class="sc">}</span><span class="vs">px`</span>)</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>                    <span class="op">.</span><span class="fu">style</span>(<span class="st">"top"</span><span class="op">,</span> <span class="vs">`</span><span class="sc">${</span>annotationY<span class="sc">}</span><span class="vs">px`</span>)</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>                    <span class="op">.</span><span class="fu">style</span>(<span class="st">"font-size"</span><span class="op">,</span> <span class="vs">`</span><span class="sc">${</span>scaler<span class="op">*</span>fontSize<span class="sc">}</span><span class="vs">px`</span>)</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>                    <span class="op">.</span><span class="fu">style</span>(<span class="st">"transform"</span><span class="op">,</span> <span class="vs">`translateY(-50%) rotate(</span><span class="sc">${</span>angle<span class="sc">}</span><span class="vs">deg)`</span>)</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>                    <span class="op">.</span><span class="fu">html</span>(<span class="vs">`</span><span class="sc">\\</span><span class="vs">(</span><span class="sc">${</span>weightAnnotation<span class="sc">}\\</span><span class="vs">)`</span>)<span class="op">;</span></span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Draw neurons and neuron annotations</span></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>    layers<span class="op">.</span><span class="fu">forEach</span>((numNeurons<span class="op">,</span> layerIndex) <span class="kw">=&gt;</span> {</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>        <span class="kw">const</span> x <span class="op">=</span> (layerIndex <span class="op">+</span> <span class="fl">0.5</span>) <span class="op">*</span> layerWidth<span class="op">;</span></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>        <span class="kw">const</span> layerAnnotation <span class="op">=</span> <span class="vs">`Layer </span><span class="sc">${</span>layerIndex<span class="sc">}</span><span class="vs">`</span><span class="op">;</span></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>        svg<span class="op">.</span><span class="fu">append</span>(<span class="st">"text"</span>)</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>            <span class="op">.</span><span class="fu">attr</span>(<span class="st">"x"</span><span class="op">,</span> x)</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>            <span class="op">.</span><span class="fu">attr</span>(<span class="st">"y"</span><span class="op">,</span> <span class="dv">20</span>) <span class="co">// Position at the top, you can adjust this value</span></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>            <span class="op">.</span><span class="fu">attr</span>(<span class="st">"text-anchor"</span><span class="op">,</span> <span class="st">"middle"</span>)</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>            <span class="op">.</span><span class="fu">attr</span>(<span class="st">"font-family"</span><span class="op">,</span> <span class="st">"Arial"</span>)</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>            <span class="op">.</span><span class="fu">attr</span>(<span class="st">"font-size"</span><span class="op">,</span> <span class="st">"16px"</span>)</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>            <span class="op">.</span><span class="fu">text</span>(layerAnnotation)<span class="op">;</span></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="kw">let</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> numNeurons<span class="op">;</span> i<span class="op">++</span>) {</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>            <span class="kw">const</span> y <span class="op">=</span> <span class="fu">calculateY</span>(layerIndex<span class="op">,</span> i<span class="op">,</span> numNeurons)<span class="op">;</span></span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>            <span class="co">// Draw neuron</span></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>            svg<span class="op">.</span><span class="fu">append</span>(<span class="st">"circle"</span>)</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>                <span class="op">.</span><span class="fu">attr</span>(<span class="st">"cx"</span><span class="op">,</span> x)</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>                <span class="op">.</span><span class="fu">attr</span>(<span class="st">"cy"</span><span class="op">,</span> y)</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>                <span class="op">.</span><span class="fu">attr</span>(<span class="st">"r"</span><span class="op">,</span> neuronRadius)</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>                <span class="op">.</span><span class="fu">attr</span>(<span class="st">"fill"</span><span class="op">,</span> <span class="st">"steelblue"</span>)<span class="op">;</span></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>            <span class="co">// Add neuron annotation</span></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>            <span class="kw">const</span> neuronAnnotation <span class="op">=</span> <span class="vs">`x_{</span><span class="sc">${</span>i <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="vs">}^{(</span><span class="sc">${</span>layerIndex<span class="sc">}</span><span class="vs">)}`</span><span class="op">;</span></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>            d3<span class="op">.</span><span class="fu">select</span>(<span class="vs">`#</span><span class="sc">${</span>div_name<span class="sc">}</span><span class="vs">`</span>)<span class="op">.</span><span class="fu">append</span>(<span class="st">"div"</span>)</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>                <span class="op">.</span><span class="fu">attr</span>(<span class="st">"class"</span><span class="op">,</span> <span class="st">"neuron-annotation"</span>)</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>                <span class="op">.</span><span class="fu">style</span>(<span class="st">"left"</span><span class="op">,</span> <span class="vs">`</span><span class="sc">${</span>x<span class="sc">}</span><span class="vs">px`</span>)</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>                <span class="op">.</span><span class="fu">style</span>(<span class="st">"top"</span><span class="op">,</span> <span class="vs">`</span><span class="sc">${</span>y<span class="sc">}</span><span class="vs">px`</span>)</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>                <span class="op">.</span><span class="fu">style</span>(<span class="st">"font-size"</span><span class="op">,</span> <span class="vs">`</span><span class="sc">${</span>fontSize<span class="sc">}</span><span class="vs">px`</span>)</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>                <span class="op">.</span><span class="fu">style</span>(<span class="st">'color'</span><span class="op">,</span> <span class="st">'white'</span>)</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>                <span class="op">.</span><span class="fu">html</span>(<span class="vs">`</span><span class="sc">\\</span><span class="vs">(</span><span class="sc">${</span>neuronAnnotation<span class="sc">}\\</span><span class="vs">)`</span>)<span class="op">;</span></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>            <span class="co">// Draw annotation for non-input neurons</span></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> (layerIndex <span class="op">&gt;</span> <span class="dv">0</span> <span class="op">&amp;&amp;</span> drawZSquares) {</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>                <span class="kw">const</span> rectX <span class="op">=</span> x <span class="op">-</span> neuronRadius <span class="op">-</span> squareSize <span class="op">-</span> <span class="dv">1</span><span class="op">;</span></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>                <span class="kw">const</span> rectY <span class="op">=</span> y <span class="op">-</span> squareSize <span class="op">/</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>                <span class="kw">const</span> squareAnnotation <span class="op">=</span> <span class="vs">`z_{</span><span class="sc">${</span>i <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="vs">}^{(</span><span class="sc">${</span>layerIndex<span class="sc">}</span><span class="vs">)}`</span><span class="op">;</span></span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>                d3<span class="op">.</span><span class="fu">select</span>(<span class="vs">`#</span><span class="sc">${</span>div_name<span class="sc">}</span><span class="vs">`</span>)<span class="op">.</span><span class="fu">append</span>(<span class="st">"div"</span>)</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>                    <span class="op">.</span><span class="fu">attr</span>(<span class="st">"class"</span><span class="op">,</span> <span class="st">"annotation"</span>)</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>                    <span class="op">.</span><span class="fu">style</span>(<span class="st">"left"</span><span class="op">,</span> <span class="vs">`</span><span class="sc">${</span>rectX<span class="sc">}</span><span class="vs">px`</span>)</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>                    <span class="op">.</span><span class="fu">style</span>(<span class="st">"top"</span><span class="op">,</span> <span class="vs">`</span><span class="sc">${</span>rectY<span class="sc">}</span><span class="vs">px`</span>)</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>                    <span class="op">.</span><span class="fu">style</span>(<span class="st">"width"</span><span class="op">,</span> <span class="vs">`</span><span class="sc">${</span>squareSize<span class="sc">}</span><span class="vs">px`</span>)</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>                    <span class="op">.</span><span class="fu">style</span>(<span class="st">"height"</span><span class="op">,</span> <span class="vs">`</span><span class="sc">${</span>squareSize<span class="sc">}</span><span class="vs">px`</span>)</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>                    <span class="op">.</span><span class="fu">style</span>(<span class="st">"line-height"</span><span class="op">,</span> <span class="vs">`</span><span class="sc">${</span>squareSize<span class="sc">}</span><span class="vs">px`</span>)</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>                    <span class="op">.</span><span class="fu">style</span>(<span class="st">"font-size"</span><span class="op">,</span> <span class="vs">`</span><span class="sc">${</span><span class="fl">0.8</span><span class="op">*</span>fontSize<span class="sc">}</span><span class="vs">px`</span>)</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a>                    <span class="op">.</span><span class="fu">html</span>(<span class="vs">`</span><span class="sc">\\</span><span class="vs">(</span><span class="sc">${</span>squareAnnotation<span class="sc">}\\</span><span class="vs">)`</span>)</span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>                    <span class="op">.</span><span class="fu">on</span>(<span class="st">"click"</span><span class="op">,</span> () <span class="kw">=&gt;</span> <span class="fu">propagateFromZSquare</span>(layerIndex<span class="op">,</span> i))<span class="op">;;</span></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>    })<span class="op">;</span></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Function to animate impulse</span></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>    <span class="kw">function</span> <span class="fu">animateImpulse</span>(startX<span class="op">,</span> startY<span class="op">,</span> endX<span class="op">,</span> endY<span class="op">,</span> duration) {</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a>        <span class="kw">const</span> impulse <span class="op">=</span> svg<span class="op">.</span><span class="fu">append</span>(<span class="st">"circle"</span>)</span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>            <span class="op">.</span><span class="fu">attr</span>(<span class="st">"cx"</span><span class="op">,</span> startX)</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>            <span class="op">.</span><span class="fu">attr</span>(<span class="st">"cy"</span><span class="op">,</span> startY)</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>            <span class="op">.</span><span class="fu">attr</span>(<span class="st">"r"</span><span class="op">,</span> <span class="dv">5</span>)</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>            <span class="op">.</span><span class="fu">attr</span>(<span class="st">"fill"</span><span class="op">,</span> <span class="st">"red"</span>)<span class="op">;</span></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>        impulse<span class="op">.</span><span class="fu">transition</span>()</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>            <span class="op">.</span><span class="fu">duration</span>(duration)</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>            <span class="op">.</span><span class="fu">attr</span>(<span class="st">"cx"</span><span class="op">,</span> endX)</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>            <span class="op">.</span><span class="fu">attr</span>(<span class="st">"cy"</span><span class="op">,</span> endY)</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>            <span class="op">.</span><span class="fu">on</span>(<span class="st">"end"</span><span class="op">,</span> () <span class="kw">=&gt;</span> impulse<span class="op">.</span><span class="fu">remove</span>())<span class="op">;</span></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Function to propagate impulse</span></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>    <span class="kw">function</span> <span class="fu">propagateImpulse</span>(layerIndex<span class="op">,</span> neuronIndex) {</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (layerIndex <span class="op">&lt;</span> layers<span class="op">.</span><span class="at">length</span> <span class="op">-</span> <span class="dv">1</span>) {</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> (<span class="kw">let</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> layers[layerIndex <span class="op">+</span> <span class="dv">1</span>]<span class="op">;</span> j<span class="op">++</span>) {</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>                <span class="kw">const</span> startX <span class="op">=</span> (layerIndex <span class="op">+</span> <span class="fl">0.5</span>) <span class="op">*</span> layerWidth<span class="op">;</span></span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a>                <span class="kw">const</span> startY <span class="op">=</span> <span class="fu">calculateY</span>(layerIndex<span class="op">,</span> neuronIndex<span class="op">,</span> layers[layerIndex])<span class="op">;</span></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>                <span class="kw">const</span> endX <span class="op">=</span> (layerIndex <span class="op">+</span> <span class="fl">1.5</span>) <span class="op">*</span> layerWidth <span class="op">-</span> neuronRadius <span class="op">-</span> squareSize <span class="op">-</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>                <span class="kw">const</span> endY <span class="op">=</span> <span class="fu">calculateY</span>(layerIndex <span class="op">+</span> <span class="dv">1</span><span class="op">,</span> j<span class="op">,</span> layers[layerIndex <span class="op">+</span> <span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>                <span class="fu">animateImpulse</span>(startX<span class="op">,</span> startY<span class="op">,</span> endX<span class="op">,</span> endY<span class="op">,</span> <span class="dv">1200</span>)<span class="op">;</span></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>                <span class="co">// Recursive call for next layer</span></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>                <span class="pp">setTimeout</span>(() <span class="kw">=&gt;</span> <span class="fu">propagateImpulse</span>(layerIndex <span class="op">+</span> <span class="dv">1</span><span class="op">,</span> j)<span class="op">,</span> <span class="dv">1000</span>)<span class="op">;</span></span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Function to propagate impulse from a Z square</span></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>    <span class="kw">function</span> <span class="fu">propagateFromZSquare</span>(layerIndex<span class="op">,</span> neuronIndex) {</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (layerIndex <span class="op">&lt;</span> layers<span class="op">.</span><span class="at">length</span> <span class="op">-</span> <span class="dv">1</span>) {</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> (<span class="kw">let</span> j <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> j <span class="op">&lt;</span> layers[layerIndex <span class="op">+</span> <span class="dv">1</span>]<span class="op">;</span> j<span class="op">++</span>) {</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a>                <span class="kw">const</span> startX <span class="op">=</span> (layerIndex <span class="op">+</span> <span class="fl">0.5</span>) <span class="op">*</span> layerWidth<span class="op">;</span></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>                <span class="kw">const</span> startY <span class="op">=</span> <span class="fu">calculateY</span>(layerIndex<span class="op">,</span> neuronIndex<span class="op">,</span> layers[layerIndex]) <span class="op">+</span> squareSize <span class="op">/</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>                <span class="kw">const</span> endX <span class="op">=</span> (layerIndex <span class="op">+</span> <span class="fl">1.5</span>) <span class="op">*</span> layerWidth <span class="op">-</span> neuronRadius <span class="op">-</span> squareSize <span class="op">-</span> <span class="dv">2</span><span class="op">;</span></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>                <span class="kw">const</span> endY <span class="op">=</span> <span class="fu">calculateY</span>(layerIndex <span class="op">+</span> <span class="dv">1</span><span class="op">,</span> j<span class="op">,</span> layers[layerIndex <span class="op">+</span> <span class="dv">1</span>])<span class="op">;</span></span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>                <span class="fu">animateImpulse</span>(startX<span class="op">,</span> startY<span class="op">,</span> endX<span class="op">,</span> endY<span class="op">,</span> <span class="dv">1200</span>)<span class="op">;</span></span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Draw neurons and neuron annotations</span></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>    layers<span class="op">.</span><span class="fu">forEach</span>((numNeurons<span class="op">,</span> layerIndex) <span class="kw">=&gt;</span> {</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (<span class="kw">let</span> i <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> i <span class="op">&lt;</span> numNeurons<span class="op">;</span> i<span class="op">++</span>) {</span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a>            <span class="kw">const</span> x <span class="op">=</span> (layerIndex <span class="op">+</span> <span class="fl">0.5</span>) <span class="op">*</span> layerWidth<span class="op">;</span></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>            <span class="kw">const</span> y <span class="op">=</span> <span class="fu">calculateY</span>(layerIndex<span class="op">,</span> i<span class="op">,</span> numNeurons)<span class="op">;</span></span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>            <span class="co">// Draw neuron</span></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>            svg<span class="op">.</span><span class="fu">append</span>(<span class="st">"circle"</span>)</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>                <span class="op">.</span><span class="fu">attr</span>(<span class="st">"cx"</span><span class="op">,</span> x)</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>                <span class="op">.</span><span class="fu">attr</span>(<span class="st">"cy"</span><span class="op">,</span> y)</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>                <span class="op">.</span><span class="fu">attr</span>(<span class="st">"r"</span><span class="op">,</span> neuronRadius)</span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>                <span class="op">.</span><span class="fu">attr</span>(<span class="st">"fill"</span><span class="op">,</span> <span class="st">"steelblue"</span>)</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>                <span class="op">.</span><span class="fu">attr</span>(<span class="st">"cursor"</span><span class="op">,</span> <span class="st">"pointer"</span>)</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>                <span class="op">.</span><span class="fu">on</span>(<span class="st">"click"</span><span class="op">,</span> () <span class="kw">=&gt;</span> <span class="fu">propagateImpulse</span>(layerIndex<span class="op">,</span> i))<span class="op">;</span></span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>    })<span class="op">;</span></span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Render MathJax</span></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a>    MathJax<span class="op">.</span><span class="fu">typesetPromise</span>()<span class="op">;</span></span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="ojs-cell-1" data-nodetype="declaration">

</div>
</div>
</div>
<section id="learning-objectives-and-pre-requisites" class="level2">
<h2 class="anchored" data-anchor-id="learning-objectives-and-pre-requisites">Learning Objectives and Pre-Requisites</h2>
<p>In this tutorial, we will explore how feedforward neural networks work. Weâ€™ll discuss neurons, layers, activation functions, and cost functions. Then, weâ€™ll see in detail how we train a neural network using backpropagation. We will derive the backpropagation formulae step-by-step and implement a neural network from scratch using Pythonâ€™s Numpy package only.</p>
<p><strong>Learning Objectives:</strong> <br></p>
<p>At the end of this tutorial, the reader should be able to:</p>
<ul>
<li>Calculate the forward pass of a neural network;</li>
<li>Calculate the backpropagation of a neural network;</li>
<li>Implement a neural network from scratch in Python;</li>
<li>Implement a neural network using PyTorch;</li>
</ul>
<p><strong>Prerequisites:</strong> <br></p>
<p>It is assumed that the reader:</p>
<ul>
<li>is proficient in computing derivatives, particularly in the application of the <em>Chain Rule</em>.</li>
<li>has some Python knowledge;</li>
<li>is able to perform matrix multiplication;</li>
</ul>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Neural Networks are certainly among the most â€œfamousâ€ models in machine learning. They power many tools that we use nowadays, from computer vision to generative models and medical applications. If this is not the first post youâ€™ve read about neural networks, you surely have seen a diagram like the one below.</p>
<div id="fig-nn-regular" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nn-regular-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><img src="images/nn1.svg" width="100%" class="figure-img"></p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nn-regular-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The usual Neural Network diagram (bias neurons omitted).
</figcaption>
</figure>
</div>
<p>When I started learning about neural networks, I found the standard diagram confusing because it doesnâ€™t explicitly show a crucial component that will be needed later for the backpropagation algorithm. Therefore, for this tutorial, we will explicitly include this component in the diagram, as shown in <a href="#fig-nn-diagram" class="quarto-xref">Figure&nbsp;2</a>.</p>
<div id="fig-nn-diagram" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nn-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><img src="images/nn2.svg" width="100%" class="figure-img"></p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nn-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: A Neural Network diagram including the linear aggregator <span class="math inline">\(z^{(l)}_{i}\)</span> (again, bias neurons omitted).
</figcaption>
</figure>
</div>
<p>Letâ€™s introduce some terminology:</p>
<ul>
<li><p><strong>Layers</strong>: This neuron network has four layers.</p>
<ul>
<li><strong>Input layer:</strong> the first layer is known as the input layer; it brings the data into the network.</li>
<li><strong>Output layer:</strong> the last layer is known as the output layer; it provides the numerical outputs of the neural network.</li>
<li><strong>Hidden layers:</strong> the layers between the input and output layers are known as the hidden layers; in this case, layers 1 and 2 are hidden layers.</li>
</ul></li>
<li><p><strong>Neurons:</strong> the blue circles are the so-called neurons; neurons send a numerical value as a signal for the neurons in the following layer.</p>
<ul>
<li>Different layers can have different numbers of neurons.</li>
<li>The signals neurons in the input layer send are the data.</li>
<li>The number of neurons in the input layer is the number of attributes in the dataset.</li>
<li><strong>Activation function:</strong> a non-linear function that specifies how neurons process the signals they receive. This function is not explicitly showed in the graph, but it is â€œinsideâ€ the neuron.</li>
</ul></li>
<li><p><strong>Weights:</strong> the weights are numerical values (positive or negative) that amplify or reduce the strength of a neuronâ€™s signal to another neuron; they are represented in the graph by the lines;</p></li>
<li><p><strong>Receptors:</strong> we will call the boxes attached to each neuron the neuronâ€™s receptor, which will collect and aggregate all the signals a neuron receives from other neurons (this is not standard language);</p></li>
</ul>
<p>Iâ€™ve always found the terminology very confusing without looking at the equations. For example, when I say that weights amplify or reduce the signal, how exactly does that happen? How exactly do receptors collect and aggregate all the signals? How do neurons process the signals passed by the receptors? Before we go over these in detail, letâ€™s review the notation we are using.</p>
</section>
<section id="notation" class="level2">
<h2 class="anchored" data-anchor-id="notation">Notation</h2>
<div id="fig-nn-notation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nn-notation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><img src="images/nn3.jpg" width="100%" class="figure-img"></p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nn-notation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: The weight <span class="math inline">\(w_{1,2}^{(1)}\)</span> connects the neurons <span class="math inline">\(X^{(0)}_1\)</span> and <span class="math inline">\(X^{(1)}_2\)</span>.
</figcaption>
</figure>
</div>
<ul>
<li><span class="math inline">\((l)\)</span> refers to the layer, and goes from 0 to <span class="math inline">\(L\)</span>, where the <span class="math inline">\(L\)</span>th layer is the output layer.</li>
<li><span class="math inline">\(n^{(l)}\)</span> is the number of neurons in layer <span class="math inline">\(l\)</span>.</li>
<li><span class="math inline">\(x^{(l)}_{k}\)</span> is the <span class="math inline">\(k\)</span>th neuron in layer <span class="math inline">\(l\)</span>.
<ul>
<li><span class="math inline">\(x^{(l)}_{i,k}\)</span> is the value of the <span class="math inline">\(k\)</span>th neuron in layer <span class="math inline">\(l\)</span> for the <span class="math inline">\(i\)</span>th training sample.</li>
</ul></li>
<li><span class="math inline">\(z^{(l)}_{k}\)</span> is the receptor of neuron <span class="math inline">\(x^{(l)}_k\)</span>.
<ul>
<li><span class="math inline">\(z^{(l)}_{i,k}\)</span> is the value of the receptor of neuron <span class="math inline">\(x^{(l)}_k\)</span> for the <span class="math inline">\(i\)</span>th training sample.</li>
</ul></li>
<li><span class="math inline">\(w_{i,j}^{(l)}\)</span> is the weight connecting the <span class="math inline">\(i\)</span>th neuron in layer <span class="math inline">\(l-1\)</span> to the <span class="math inline">\(j\)</span>th neuron in layer <span class="math inline">\(l\)</span>.</li>
<li><span class="math inline">\(b^{(l)}_k\)</span> the bias term added by the receptor of neuron <span class="math inline">\(k\)</span> in layer <span class="math inline">\(l\)</span>.</li>
</ul>
<p>Since we have a ton of weights, it is helpful for us to organize them into matrices. We will have one weight matrix per layer (except for layer 0). We will denote the matrices as <span class="math inline">\({\bf{W}}^{(l)}\)</span>. <a href="#fig-nn-matrix-weights" class="quarto-xref">Figure&nbsp;4</a> illustrates how the weights are organized into matrices for our example neural network.</p>
<div id="fig-nn-matrix-weights" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nn-matrix-weights-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><img src="images/nn4.svg" width="100%" class="figure-img"></p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nn-matrix-weights-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: The weight <span class="math inline">\(w_{1,2}^{(1)}\)</span> connects the neurons <span class="math inline">\(X^{(0)}_1\)</span> and <span class="math inline">\(X^{(1)}_2\)</span>.
</figcaption>
</figure>
</div>
<p>The matrix <span class="math inline">\({\bf{W}}^{(l)}\)</span> contains the weights connecting neurons in layer <span class="math inline">\(l-1\)</span> to neurons in layer <span class="math inline">\(l\)</span>. It has <span class="math inline">\(n^{(l-1)}\)</span> rows and <span class="math inline">\(n^{(l)}\)</span> columns. The <span class="math inline">\(i\)</span>th row of <span class="math inline">\({\bf{W}}^{(l)}\)</span> are all weights â€œleavingâ€ neuron <span class="math inline">\(i\)</span> from layer <span class="math inline">\(l-1\)</span>. The <span class="math inline">\(j\)</span>th column of <span class="math inline">\({\bf{W}}^{(l)}\)</span> are all the weights â€œarrivingâ€ in neuron <span class="math inline">\(j\)</span> in layer <span class="math inline">\(l-1\)</span>. <a href="#fig-weight-matrix-shape" class="quarto-xref">Figure&nbsp;5</a> illustrates these points.</p>
<div id="fig-weight-matrix-shape" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-weight-matrix-shape-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><img src="images/nn5.jpg" width="100%" class="figure-img"></p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-weight-matrix-shape-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Shape of matrix <span class="math inline">\({\bf{W}}^{(1)}\)</span>.
</figcaption>
</figure>
</div>
<p>For example, the second row of <span class="math inline">\({\bf{W}}^{(2)}\)</span> has all the weights leaving neuron 2 from layer 1, as shown in <a href="#fig-weight-matrix-row" class="quarto-xref">Figure&nbsp;6</a>; while the second column has all the weights arriving at neuron 2 in layer 2, as illustrated in <a href="#fig-weight-matrix-column" class="quarto-xref">Figure&nbsp;7</a>.</p>
<div id="fig-weight-matrix-row" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-weight-matrix-row-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><img src="images/nn6.svg" width="100%" class="figure-img"></p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-weight-matrix-row-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: The second row of <span class="math inline">\({\bf{W}}^{(2)}\)</span> has all weights â€œleavingâ€ neuron 2 in layer 1.
</figcaption>
</figure>
</div>
<div id="fig-weight-matrix-column" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-weight-matrix-column-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><img src="images/nn7.svg" width="100%" class="figure-img"></p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-weight-matrix-column-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: The second column of <span class="math inline">\({\bf{W}}^{(2)}\)</span> has all weights â€œarrivingâ€ at neuron 2 in layer 2.
</figcaption>
</figure>
</div>
<p>Now that we understand the notation, we are ready to introduce the necessary equations.</p>
<ul>
<li><strong>Receptors</strong>: The value of the <span class="math inline">\(k\)</span>th receptor in layer <span class="math inline">\(l\)</span> for the <span class="math inline">\(i\)</span>th training sample is given by: <span class="math display">\[z^{(l)}_{i, k} =\sum_{j=1}^{n^{(l-1)}} w^{(l)}_{j, k}x^{(l-1)}_{i,j} + b^{(l)}_k,\quad l=1,...,L, \quad \text{and} \quad j=1,...,n^{(l)}\]</span></li>
<li><strong>Neurons</strong> (for the <span class="math inline">\(i\)</span>th training sample): <span class="math display">\[x^{(l)}_{i, k}=a\left(z^{(l)}_{i, k}\right),\quad l=1,...,L, \quad \text{and} \quad j=1,...,n^{(l)}\]</span> where <span class="math inline">\(a\)</span> is a non-linear function called <strong>activation function</strong>. We will discuss activation functions in more detail later. For now, we will use <span class="math inline">\(a(x)=\max\left\{0, x\right\}\)</span>.
<ul>
<li>Note: for the input layer, <span class="math inline">\(l=0\)</span>, <span class="math inline">\(x^{(0)}_j\)</span> is just the feature <span class="math inline">\(j\)</span> of the input vector.</li>
</ul></li>
</ul>
<p>Okay, I agree; the notation is heavy. We have a lot of things to keep track of, such as layers, receptors, neurons, and weights, so we need a lot of symbols and indices. For this reason, I encourage the reader to go back to <a href="#fig-nn-diagram" class="quarto-xref">Figure&nbsp;2</a>, pick a neuron in a hidden layer, and write down the equations for that neuron while identifying the elements being used in the diagram.</p>
<div id="exm-forward-pass" class="theorem example">
<p><span class="theorem-title"><strong>Example 1</strong></span> For us to go through an example of the feedforward part of the neural network, let us get some synthetic data with two features as well as define some values for the weights.</p>
<div id="fig-synthetic-data" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-synthetic-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><img src="images/nn8.jpg" width="80%" class="figure-img"></p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-synthetic-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: Some synthetic data and weights for our neural network.
</figcaption>
</figure>
</div>
<p>Now, we can calculate the forward pass of the neural network. Letâ€™s do it for the first row in our data, i.e., for the input vector <span class="math inline">\(x=(6.1, 8.7)\)</span>.</p>
<div class="columns">
<div class="column" style="width:45%;">
<ul>
<li><p>Receptors in Layer 1: <span class="math display">\[z^{(1)}_1 =\sum_{i=1}^{2} w^{(1)}_{i, 1}x^{(0)}_i = -0.314\]</span> <span class="math display">\[z^{(1)}_2 =\sum_{i=1}^{2} w^{(1)}_{i, 2}x^{(0)}_i =  0.818\]</span> <span class="math display">\[z^{(1)}_3 =\sum_{i=1}^{2} w^{(1)}_{i, 3}x^{(0)}_i = -0.417\]</span></p></li>
<li><p>Receptors in Layer 2: <span class="math display">\[z^{(2)}_1 =\sum_{i=1}^{3} w^{(2)}_{i, 1}x^{(1)}_i = -0.07362\]</span> <span class="math display">\[z^{(2)}_2 =\sum_{i=1}^{3} w^{(2)}_{i, 2}x^{(1)}_i =  0.02454\]</span></p></li>
<li><p>Receptor in Layer 3: <span class="math display">\[z^{(3)}_1 =\sum_{i=1}^{2} w^{(3)}_{i, 1}x^{(1)}_i = 0.0017178\]</span></p></li>
</ul>
</div><div class="column" style="width:5%;">
<!-- empty column to create gap -->
</div><div class="column" style="width:45%;">
<ul>
<li><p>Neurons in Layer 1: <span class="math display">\[x^{(1)}_1 = \max\left\{0, z^{(1)}_1\right\}  = 0\ \ \ \ \ \ \textcolor{white}{\sum_{i=1}^{2}}\]</span> <span class="math display">\[x^{(1)}_2 = \max\left\{0, z^{(1)}_2\right\} = 0.818 \textcolor{white}{\sum_{i=1}^{2}}\]</span> <span class="math display">\[x^{(1)}_3 = \max\left\{0, z^{(1)}_3\right\} = 0\ \ \ \ \ \ \textcolor{white}{\sum_{i=1}^{2}}\]</span></p></li>
<li><p>Neurons in Layer 2: <span class="math display">\[x^{(2)}_1 = \max\left\{0, z^{(2)}_1\right\} = 0\ \ \ \ \ \ \ \ \ \ \textcolor{white}{\sum_{i=1}^{2}}\]</span> <span class="math display">\[x^{(2)}_2 = \max\left\{0, z^{(2)}_2\right\} = 0.02454 \textcolor{white}{\sum_{i=1}^{2}}\]</span></p></li>
<li><p>Neuron in Layer 3: <span class="math display">\[x^{(3)}_1 = z^{(3)}_1 = 0.0017178\]</span></p></li>
</ul>
</div>
</div>
<p>Letâ€™s now visualize this result in the diagram. We will use two decimal places due to space constraints.</p>
<div id="fig-forward-pass" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-forward-pass-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><img src="images/nn9.svg" width="100%" class="figure-img"></p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-forward-pass-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Forward pass of the neural network for the input vector <span class="math inline">\(x=(6.1, 8.7)\)</span>.
</figcaption>
</figure>
</div>
</div>
<p>Congratulations! You have completed the forward pass of the neural network.</p>
<p>Iâ€™ll just note here that the activation function used in the output layer usually changes according to the problem. For example, for regression problems, a common choice is the identity function <span class="math inline">\(f(z) = z\)</span>. If we used <span class="math inline">\(f(z) = \max\left\{0, z\right\}\)</span>, we would never be able to predict a negative value, an undesirable property.</p>
<section id="matrix-notation" class="level3">
<h3 class="anchored" data-anchor-id="matrix-notation">Matrix Notation</h3>
<p>To implement a Neural Network in Python from scratch, we will need to use the highly optimized Numpyâ€™s vectorization; so, letâ€™s introduce the matrix notation here. The matrix notation also has the advantage of simplifying the steps. Note that everything is almost exactly the same; the only difference is that, with the matrix notation, we will be considering the entire dataset.</p>
<p>We will denote matrices with capital bold letters (e.g., <span class="math inline">\(\bf{X}\)</span>, <span class="math inline">\({\bf{W}}^{(1)}\)</span>), vectors as lowercase bold letters (e.g., <span class="math inline">\({\bf{x}}_1\)</span>). Also, vectors are always column vectors (multiple rows, one column). Here are the equations in matrix format:</p>
<ul>
<li><p>Receptors: <span id="eq-matrix-receptor"><span class="math display">\[
{\bf{Z}}^{(l)} = {\mathbf{X}^{(l-1)}} \mathbf{W}^{(l)} + \mathbf{1}_n\left(\mathbf{b}^{(l)}\right)^T
\tag{1}\]</span></span> where <span class="math inline">\(\mathbf{1}_n\)</span> is a column vector of ones with <span class="math inline">\(n\)</span> rows and <span class="math inline">\(T\)</span> stands for transpose. The operation <span class="math inline">\(\mathbf{1}\left(\mathbf{b}^{(l)}\right)^T\)</span> is Numpyâ€™s broadcast.</p></li>
<li><p>Neurons: <span id="eq-matrix-neuron"><span class="math display">\[
{\bf{X}}^{(l)} = a\left({\bf{Z}}^{(l)}\right)
\tag{2}\]</span></span> where <span class="math inline">\(a\left({\bf{Z}}^{(l)}\right)\)</span> means we apply the activation function <span class="math inline">\(a\)</span> to every single element of <span class="math inline">\({\bf{Z}}^{(l)}\)</span>.</p></li>
</ul>
<p>The notation becomes much simpler, doesnâ€™t it? Letâ€™s take a closer look at <span class="math inline">\({\bf{Z}}^{(l)}\)</span> in <a href="#fig-matrix-receptor" class="quarto-xref">Figure&nbsp;10</a> .</p>
<div id="fig-matrix-receptor" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-matrix-receptor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><img src="images/nn10.jpg" width="100%" class="figure-img"></p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-matrix-receptor-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: The representation of <span class="math inline">\({\bf{Z}}^{(l)}\)</span>.
</figcaption>
</figure>
</div>
<p>Now, letâ€™s go through <a href="#exm-forward-pass" class="quarto-xref">Example&nbsp;1</a> again, but this time, we will use matrix notation.</p>
<div id="exm-forward-pass-matrix" class="theorem example">
<p><span class="theorem-title"><strong>Example 2</strong></span> We will use the same data and weights as before, as shown in <a href="#fig-synthetic-data" class="quarto-xref">Figure&nbsp;8</a>. The forward pass involves calculating the values of the receptors and neurons for each layer of the network and each input vector. We will start by calculating <span class="math inline">\({\bf{Z}}^{(1)}\)</span>, <span class="math inline">\({\bf{X}}^{(1)}\)</span>, <span class="math inline">\({\bf{Z}}^{(2)}\)</span>, <span class="math inline">\({\bf{X}}^{(2)}\)</span>, <span class="math inline">\({\bf{Z}}^{(3)}\)</span>, and <span class="math inline">\({\bf{X}}^{(3)}\)</span> for all input vectors. We will first do it mathematically, then we will implement it in Python.</p>
<ul>
<li>We start by calculating <span class="math inline">\({\bf{Z}}^{(1)}\)</span> and <span class="math inline">\({\bf{X}}^{(1)}\)</span>:</li>
</ul>
<div id="fig-matrix-example-layer1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-matrix-example-layer1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><img src="images/nn11.svg" width="100%" class="figure-img"></p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-matrix-example-layer1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Values of the receptors and neurons in the first layer for all input vectors.
</figcaption>
</figure>
</div>
<ul>
<li>Using the results from the first layer, we can calculate <span class="math inline">\({\bf{Z}}^{(2)}\)</span> and <span class="math inline">\({\bf{X}}^{(2)}\)</span>:</li>
</ul>
<div id="fig-matrix-example-layer2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-matrix-example-layer2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><img src="images/nn12.svg" width="100%" class="figure-img"></p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-matrix-example-layer2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;12: Values of the receptors and neurons in the second layer for all input vectors.
</figcaption>
</figure>
</div>
<ul>
<li>Using the results from the second layer, we can calculate <span class="math inline">\({\bf{Z}}^{(3)}\)</span> and <span class="math inline">\({\bf{X}}^{(3)}\)</span>:</li>
</ul>
<div id="fig-matrix-example-layer3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-matrix-example-layer3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><img src="images/nn13.svg" width="85%" class="figure-img"></p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-matrix-example-layer3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;13: Values of the receptors and neurons in the third layer for all input vectors.
</figcaption>
</figure>
</div>
</div>
<p>Now that we have the results for all layers and all input vectors, we can implement the forward pass of the neural network in Python. We start by generating the data and initializing the weights.</p>
<div>
<div id="pyodide-1" class="exercise-cell">

</div>
<script type="pyodide-1-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWV9LCJjb2RlIjoiIyBXZSBuZWVkIHRoZSBudW1weSBsaWJyYXJ5XG5pbXBvcnQgbnVtcHkgYXMgbnBcblxuIyBDcmVhdGluZyBhIHJhbmRvbSBudW1iZXIgZ2VuZXJhdG9yIChzZWVkPTEgZm9yIHJlcHJvZHVjaWJpbGl0eSlcbnJuZyA9IG5wLnJhbmRvbS5kZWZhdWx0X3JuZyhzZWVkPTEpXG5cbiMgQ3JlYXRpbmcgdGhlIGZlYXR1cmVzIGRhdGEgXG5YID0gbnAucm91bmQocm5nLnVuaWZvcm0oMywgOSwgc2l6ZT0oMTAsMikpLCAxKVxuXG4jIENyZWF0aW5nIHJlc3BvbnNlIGRhdGFcbnkgPSBucC5yb3VuZChucC5zdW0oWCoqMiwgYXhpcz0xKSArIHJuZy5ub3JtYWwobG9jPTAsIHNjYWxlID0gMywgc2l6ZT0xMCkpXG5cbiMgSW5pdGlhbGl6aW5nIHdlaWdodHNcblcxID0gbnAuYXJyYXkoW1stMC4wOCwgMC4wMiwgMC4wNl0sIFswLjAyLCAwLjA4LCAtMC4wOV1dKVxuVzIgPSBucC5hcnJheShbWzAuMDEsIC0wLjAxXSwgWy0wLjA5LCAwLjAzXSwgWzAuMDcsIDAuMDJdXSlcblczID0gbnAuYXJyYXkoW1stMC4wNV0sIFswLjA3XV0pXG5cbiMgUHJpbnRpbmcgdGhlIGRhdGEgYW5kIHdlaWdodHNcbnByaW50KGYnWDoge1h9XFxuXFxuWToge1l9XFxuXFxuVzE6IHtXMX1cXG5cXG5XMjoge1cyfVxcblxcblczOiB7VzN9JykifQ==
</script>
</div>
<p>Great, this matches the data and weights in <a href="#fig-synthetic-data" class="quarto-xref">Figure&nbsp;8</a>.</p>
<p>Next, we can calculate <span class="math inline">\({\bf{Z}}^{(1)}\)</span>, <span class="math inline">\({\bf{Z}}^{(2)}\)</span>, and <span class="math inline">\({\bf{Z}}^{(3)}\)</span> and <span class="math inline">\({\bf{X}}^{(1)}\)</span>, <span class="math inline">\({\bf{X}}^{(2)}\)</span>, and <span class="math inline">\({\bf{X}}^{(3)}\)</span></p>
<div>
<div id="pyodide-2" class="exercise-cell">

</div>
<script type="pyodide-2-contents">
eyJhdHRyIjp7ImVkaXQiOnRydWUsImV2YWwiOnRydWV9LCJjb2RlIjoiWjEgPSBYIEAgVzFcblgxID0gbnAuZm1heCgwLCBaMSlcblxuWjIgPSBYMSBAIFcyXG5YMiA9IG5wLmZtYXgoMCwgWjIpXG5cblozID0gWDIgQCBXM1xuWDMgPSBucC5mbWF4KDAsIFozKVxuXG4jIFByaW50aW5nIHRoZSByZXN1bHRzXG5wcmludChmJ1oxOiB7WjF9XFxuXFxuWDE6IHtYMX1cXG5cXG5aMjoge1oyfVxcblxcblgyOiB7WDJ9XFxuXFxuWjM6IHtaM31cXG5cXG5YMzoge1gzfScpIn0=
</script>
</div>
<p>We successfully calculated the values of the receptors and neurons for all layers and input vectors using NumPyâ€™s vectorization.</p>
<p>By now, you should have a clear understanding of how the forward pass operates. If not, do not move to the next section. Practice a little more with the information above. There is a lot going on; the notation is heavy, and the matrix notation is a bit confusing at first. Once you are comfortable with the forward pass, we can move on to the backpropagation algorithm. Backpropagation is the algorithm we use to train the neural network. Without training, the output of the neural network is meaningless.</p>
</section>
</section>
<section id="backpropagation" class="level2">
<h2 class="anchored" data-anchor-id="backpropagation">Backpropagation</h2>
<p>Backpropagation is a critical algorithm employed in the training of neural networks. Its foundation lies in the chain rule of calculus, which allows us to compute how changes in the weights affect the overall performance of the network. The algorithm is named for its distinctive process of error propagation. The algorithm transmits the error from the output layer back through the various layers of the network all the way to the input layer. During this backward pass, the algorithm essentially assesses how much each weight contributed to the overall error, allowing for precise adjustments to be made. By updating these weights using the calculated gradients, the network can gradually improve its predictions and performance.</p>
<section id="measuring-the-error-the-cost-function" class="level3">
<h3 class="anchored" data-anchor-id="measuring-the-error-the-cost-function">Measuring the error: the cost function</h3>
<p>Before we delve into the backpropagation algorithm, we need to define a cost function. The cost function quantifies how inaccurate the networkâ€™s predictions are. The goal of the training process is to minimize this cost function.</p>
<p>For example, for regression, a commonly used cost function is:</p>
<p><span id="eq-cost-mse"><span class="math display">\[
J(\mathbf{w}|\mathbf{X}, \mathbf{y}) = \frac{1}{2n}\sum_{i=1}^{n} \left(\hat{y}_i - y_i\right)^2
\tag{3}\]</span></span></p>
<p>where <span class="math inline">\(\hat{y}_i\)</span> is the predicted value for the <span class="math inline">\(i\)</span>th sample, <span class="math inline">\(y_i\)</span> is the true value, and <span class="math inline">\(n\)</span> is the number of samples. Note that the cost function depends on the weights <span class="math inline">\(\mathbf{w}\)</span>, the input data <span class="math inline">\(\mathbf{X}\)</span>, and the true values <span class="math inline">\(\mathbf{y}\)</span>. But the input data and true values are fixed (it is the data we have); we can only change the weights. Classification problems have different cost functions, such as the cross-entropy loss (letâ€™s not worry about this for now).</p>
</section>
<section id="the-backpropagation-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="the-backpropagation-algorithm">The backpropagation algorithm</h3>
<p>The entire idea of backpropagation is to calculate the gradient of the cost function with respect to the weights. To do this, we need to pass through the elements of the network in reverse order. We will start by calculating the gradient of the cost function with respect to the neurons in the output layer. Then, we will calculate the gradient of the cost function with respect to the receptors in the output layer. Finally, we will calculate the gradient of the cost function with respect to the weights.</p>
<p>To visualize the backpropagation step-by-step, let us know so we can update our neural network diagram with the cost function, as shown in <a href="#fig-nn-diagram-cost" class="quarto-xref">Figure&nbsp;14</a>.</p>
<div id="fig-nn-diagram-cost" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nn-diagram-cost-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><img src="images/nn14.jpg" width="100%" class="figure-img"></p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nn-diagram-cost-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;14: Neural Network diagram with cost function.
</figcaption>
</figure>
</div>
<p>For now, letâ€™s do this for a single input vector (as if our data matrix <span class="math inline">\(X\)</span> had only one row). Remember, the weights are the only thing we can change; the data and true values are fixed. So, we want to calculate the gradient of the cost function with respect to the weights. Letâ€™s start with the output layer and calculate the derivative of <span class="math inline">\(w_{1,1}^{(3)}\)</span>. We will calculate this derivative using the chain rule:</p>
<ol type="1">
<li><p>First, we calculate the derivative of the cost function with respect to the neuron <span class="math inline">\(x^{(3)}_1\)</span>: <span class="math inline">\(\frac{\partial J}{\partial x^{(3)}_1}\)</span>.</p></li>
<li><p>Then, we calculate the derivative of the neuron <span class="math inline">\(x^{(3)}_1\)</span> with respect to the receptor <span class="math inline">\(z^{(3)}_1\)</span>: <span class="math inline">\(\frac{\partial x^{(3)}_1}{\partial z^{(3)}_1}\)</span>.</p></li>
<li><p>Finally, we calculate the derivative of the receptor <span class="math inline">\(z^{(3)}_1\)</span> with respect to the weight <span class="math inline">\(w_{1,1}^{(3)}\)</span>: <span class="math inline">\(\frac{\partial z^{(3)}_1}{\partial w_{1,1}^{(3)}}\)</span></p></li>
</ol>
<p>Then, we combine these using the chain rule: <span class="math display">\[
\frac{\partial J}{\partial w_{1,1}^{(3)}} = \frac{\partial J}{\partial x^{(3)}_1}\frac{\partial x^{(3)}_1}{\partial z^{(3)}_1}\frac{\partial z^{(3)}_1}{\partial w_{1,1}^{(3)}}
\]</span></p>
<p><a href="#fig-backprop-w1-1-3" class="quarto-xref">Figure&nbsp;15</a> illustrates the calculation of the derivative of <span class="math inline">\(w_{1,1}^{(3)}\)</span>.</p>
<div id="fig-backprop-w1-1-3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-backprop-w1-1-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><img src="images/nn15.jpg" width="100%" class="figure-img"></p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-backprop-w1-1-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;15: Chain rule for the derivative of <span class="math inline">\(w_{1,1}^{(3)}\)</span>.
</figcaption>
</figure>
</div>
<p>All that is left for us to do now is to calculate the derivatives. To be able to do this, we need to specify the cost and activation functions. We will use the mean squared error cost function and the ReLU activation function we have used so far.</p>
<p><span class="math display">\[
\frac{\partial J}{\partial x^{(3)}_1} = \frac{\partial}{\partial x^{(3)}_1}\left[\frac{1}{2}\left(x^{(3)}_1 - y_1\right)^2\right] = x^{(3)}_1 - y_1
\]</span></p>
<p>Next, remember we are using the identity function as the activation function for the last layer since this is a regression problem, i.e., <span class="math inline">\(x^{(3)}_1=z^{(3)}_1\)</span>. <span class="math display">\[
\frac{\partial x^{(3)}_1}{\partial z^{(3)}_1} = \frac{\partial}{\partial z^{(3)}_1}z^{(3)}_1 = 1
\]</span></p>
<p>Finally, <span class="math display">\[
\frac{\partial z^{(3)}_1}{\partial w_{1,1}^{(3)}} = \frac{\partial}{\partial w_{1,1}^{(3)}}\left[\sum_{i=1}^{2} w^{(3)}_{i, 1}x^{(2)}_i\right] = x^{(2)}_1
\]</span></p>
<p>Note the presence of <span class="math inline">\(x^{(2)}_1\)</span> and <span class="math inline">\(x^{(3)}_1\)</span> in the derivatives. These are the values obtained in the forward pass. The diagram <a href="#fig-backprop-w1-1-3-values" class="quarto-xref">Figure&nbsp;16</a> illustrates the values of the derivatives for the example we have been working on.</p>
<div id="fig-backprop-w1-1-3-values" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-backprop-w1-1-3-values-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><img src="images/nn16.jpg" width="100%" class="figure-img"></p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-backprop-w1-1-3-values-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;16: Values of the derivatives for the calculation of <span class="math inline">\(\frac{\partial J}{\partial w_{1,1}^{(3)}}\)</span>. (The values here are rounded. For more precise values check the first row of <code>Z3</code>, <code>X3</code>, and <code>X2</code> in the python code above.)
</figcaption>
</figure>
</div>
<p>Therefore, the derivative of <span class="math inline">\(w_{1,1}^{(3)}\)</span> is: <span class="math display">\[
\frac{\partial J}{\partial w_{1,1}^{(3)}} = (x^{(3)}_1 - y_1) \times 1 \times x^{(2)}_1 = (0.0017178 - 113) \times 1 \times 0 = 0
\]</span></p>
<p>Similarly, we can calculate the derivative of <span class="math inline">\(w_{2,1}^{(3)}\)</span>:</p>
<div id="fig-backprop-w2-1-3" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-backprop-w2-1-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><img src="images/nn17.jpg" width="100%" class="figure-img"></p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-backprop-w2-1-3-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;17: Chain rule for the derivative of <span class="math inline">\(w_{2,1}^{(3)}\)</span>.
</figcaption>
</figure>
</div>
<p><span class="math display">\[
\frac{\partial J}{\partial w_{2,1}^{(3)}} = (0.0017178 - 113) \times 1 \times 0.2 = -22.6
\]</span></p>
<p>Hopefully, at this point, you are convinced that we can calculate the derivatives of the output layer weights. Now, letâ€™s see how to calculate the derivative of a weight in the hidden layers. We will start by calculating the derivative of <span class="math inline">\(w_{3,2}^{(2)}\)</span>. <a href="#fig-backprop-w3-2-2" class="quarto-xref">Figure&nbsp;18</a> illustrates the calculation of the derivative of <span class="math inline">\(w_{3,2}^{(2)}\)</span>.</p>
<div id="fig-backprop-w3-2-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-backprop-w3-2-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><img src="images/nn18.jpg" width="100%" class="figure-img"></p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-backprop-w3-2-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;18: Chain rule for the derivative of <span class="math inline">\(w_{3,2}^{(2)}\)</span>.
</figcaption>
</figure>
</div>
<p>The key aspect here is for you to see that we just â€œexpandâ€ the path (note the blue path in <a href="#fig-backprop-w3-2-2" class="quarto-xref">Figure&nbsp;18</a>). The red part of the path has already been calculated in the previous step, which is the derivative of <span class="math inline">\(w_{2,1}^{(3)}\)</span>. Therefore, we just need to multiply the partial derivatives in the blue part of the path to get the derivative of <span class="math inline">\(w_{3,2}^{(2)}\)</span>.</p>
<p><span class="math display">\[
\frac{\partial J}{\partial w_{3,2}^{(2)}} = \frac{\partial J}{\partial x^{(3)}_1}\frac{\partial x^{(3)}_1}{\partial z^{(3)}_1}\frac{\partial z^{(3)}_1}{\partial x^{(2)}_1}\frac{\partial x^{(2)}_1}{\partial z^{(2)}_2}\frac{\partial z^{(2)}_2}{\partial w_{3,2}^{(2)}}
\]</span></p>
<!-- $$
\frac{\partial x^{(3)}_1}{\partial z^{(3)}_1} = \frac{\partial}{\partial z^{(3)}_1}\left[\max\left\{0, z^{(3)}_1\right\}\right] = \begin{cases} 1, & \text{if } z^{(3)}_1 > 0 \\ 0, & \text{otherwise} \end{cases}
$$ -->
</section>
</section>
<section id="implementation" class="level2">
<h2 class="anchored" data-anchor-id="implementation">Implementation</h2>
<section id="from-scratch-with-numpy" class="level3">
<h3 class="anchored" data-anchor-id="from-scratch-with-numpy">From scratch with Numpy</h3>
</section>
<section id="using-pytorch" class="level3">
<h3 class="anchored" data-anchor-id="using-pytorch">Using Pytorch</h3>


<script type="pyodide-data">
eyJvcHRpb25zIjp7ImVudiI6eyJQTE9UTFlfUkVOREVSRVIiOiJwbG90bHlfbWltZXR5cGUifSwiaW5kZXhVUkwiOiJodHRwczovL2Nkbi5qc2RlbGl2ci5uZXQvcHlvZGlkZS92MC4yNy4wL2Z1bGwvIn0sInBhY2thZ2VzIjp7InBrZ3MiOlsicHlvZGlkZV9odHRwIiwibWljcm9waXAiLCJpcHl0aG9uIl19fQ==
</script>
<script type="ojs-module-contents">
{"contents":[{"cellName":"pyodide-2","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_2 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-2-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-2-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_2 = pyodideOjs.process(_pyodide_editor_2, {});\n"},{"cellName":"pyodide-1","inline":false,"methodName":"interpret","source":"viewof _pyodide_editor_1 = {\n  const { PyodideExerciseEditor, b64Decode } = window._exercise_ojs_runtime;\n\n  const scriptContent = document.querySelector(`script[type=\\\"pyodide-1-contents\\\"]`).textContent;\n  const block = JSON.parse(b64Decode(scriptContent));\n\n  const options = Object.assign({ id: `pyodide-1-contents` }, block.attr);\n  const editor = new PyodideExerciseEditor(\n    pyodideOjs.pyodidePromise,\n    block.code,\n    options\n  );\n\n  return editor.container;\n}\n_pyodide_value_1 = pyodideOjs.process(_pyodide_editor_1, {});\n"},{"cellName":"pyodide-prelude","inline":false,"methodName":"interpretQuiet","source":"pyodideOjs = {\n  const {\n    PyodideEvaluator,\n    PyodideEnvironmentManager,\n    setupPython,\n    startPyodideWorker,\n    b64Decode,\n    collapsePath,\n  } = window._exercise_ojs_runtime;\n\n  const statusContainer = document.getElementById(\"exercise-loading-status\");\n  const indicatorContainer = document.getElementById(\"exercise-loading-indicator\");\n  indicatorContainer.classList.remove(\"d-none\");\n\n  let statusText = document.createElement(\"div\")\n  statusText.classList = \"exercise-loading-details\";\n  statusText = statusContainer.appendChild(statusText);\n  statusText.textContent = `Initialise`;\n\n  // Hoist indicator out from final slide when running under reveal\n  const revealStatus = document.querySelector(\".reveal .exercise-loading-indicator\");\n  if (revealStatus) {\n    revealStatus.remove();\n    document.querySelector(\".reveal > .slides\").appendChild(revealStatus);\n  }\n\n  // Make any reveal slides with live cells scrollable\n  document.querySelectorAll(\".reveal .exercise-cell\").forEach((el) => {\n    el.closest('section.slide').classList.add(\"scrollable\");\n  })\n\n  // Pyodide supplemental data and options\n  const dataContent = document.querySelector(`script[type=\\\"pyodide-data\\\"]`).textContent;\n  const data = JSON.parse(b64Decode(dataContent));\n\n  // Grab list of resources to be downloaded\n  const filesContent = document.querySelector(`script[type=\\\"vfs-file\\\"]`).textContent;\n  const files = JSON.parse(b64Decode(filesContent));\n\n  let pyodidePromise = (async () => {\n    statusText.textContent = `Downloading Pyodide`;\n    const pyodide = await startPyodideWorker(data.options);\n\n    statusText.textContent = `Downloading package: micropip`;\n    await pyodide.loadPackage(\"micropip\");\n    const micropip = await pyodide.pyimport(\"micropip\");\n    await data.packages.pkgs.map((pkg) => () => {\n      statusText.textContent = `Downloading package: ${pkg}`;\n      return micropip.install(pkg);\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n    await micropip.destroy();\n\n    // Download and install resources\n    await files.map((file) => async () => {\n      const name = file.substring(file.lastIndexOf('/') + 1);\n      statusText.textContent = `Downloading resource: ${name}`;\n      const response = await fetch(file);\n      if (!response.ok) {\n        throw new Error(`Can't download \\`${file}\\`. Error ${response.status}: \"${response.statusText}\".`);\n      }\n      const data = await response.arrayBuffer();\n\n      // Store URLs in the cwd without any subdirectory structure\n      if (file.includes(\"://\")) {\n        file = name;\n      }\n\n      // Collapse higher directory structure\n      file = collapsePath(file);\n\n      // Create directory tree, ignoring \"directory exists\" VFS errors\n      const parts = file.split('/').slice(0, -1);\n      let path = '';\n      while (parts.length > 0) {\n        path += parts.shift() + '/';\n        try {\n          await pyodide.FS.mkdir(path);\n        } catch (e) {\n          if (e.name !== \"ErrnoError\") throw e;\n          if (e.errno !== 20) {\n            const errorTextPtr = await pyodide._module._strerror(e.errno);\n            const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n            throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n          }\n        }\n      }\n\n      // Write this file to the VFS\n      try {\n        return await pyodide.FS.writeFile(file, new Uint8Array(data));\n      } catch (e) {\n        if (e.name !== \"ErrnoError\") throw e;\n        const errorTextPtr = await pyodide._module._strerror(e.errno);\n        const errorText = await pyodide._module.UTF8ToString(errorTextPtr);\n        throw new Error(`Filesystem Error ${e.errno} \"${errorText}\".`);\n      }\n    }).reduce((cur, next) => cur.then(next), Promise.resolve());\n\n    statusText.textContent = `Pyodide environment setup`;\n    await setupPython(pyodide);\n\n    statusText.remove();\n    if (statusContainer.children.length == 0) {\n      statusContainer.parentNode.remove();\n    }\n    return pyodide;\n  })().catch((err) => {\n    statusText.style.color = \"var(--exercise-editor-hl-er, #AD0000)\";\n    statusText.textContent = err.message;\n    //indicatorContainer.querySelector(\".spinner-grow\").classList.add(\"d-none\");\n    throw err;\n  });\n\n  // Keep track of initial OJS block render\n  const renderedOjs = {};\n\n  const process = async (context, inputs) => {\n    const pyodide = await pyodidePromise;\n    const evaluator = new PyodideEvaluator(pyodide, context);\n    await evaluator.process(inputs);\n    return evaluator.container;\n  }\n\n  return {\n    pyodidePromise,\n    renderedOjs,\n    process,\n  };\n}\n"}]}
</script>
<div id="exercise-loading-indicator" class="exercise-loading-indicator d-none d-flex align-items-center gap-2">
<div id="exercise-loading-status" class="d-flex gap-2">

</div>
<div class="spinner-grow spinner-grow-sm">

</div>
</div>
<script type="vfs-file">
W10=
</script>
</section>
</section>

</main> <!-- /main -->
<script type="ojs-module-contents">
{"contents":[{"methodName":"interpret","cellName":"ojs-cell-1","inline":false,"source":"function createNeuralNetwork(layers, div_name, drawZSquares=true, neuronRadius = 20, squareSize = 20, width = 1000, height = 400) {\n\n    const layerWidth = width / layers.length;\n    //const fontSize = neuronRadius / 2; // Set font size proportional to neuron radius\n    const fontSize = 14;\n    squareSize = drawZSquares ? squareSize : -2;\n    \n    // Clear previous content\n    //d3.select(`#${div_name}`).html('');\n\n    // Create SVG element for lines and neurons\n    const svg = d3.select(`#${div_name}`).append(\"svg\")\n        .attr(\"width\", width)\n        .attr(\"height\", height)\n        .style(\"position\", \"absolute\");\n\n    // Function to calculate Y positions\n    const calculateY = (layerIndex, nodeIndex, totalNodes) => {\n        const spacing = height / (totalNodes + 1);\n        return (nodeIndex + 1) * spacing;\n    };\n\n    // Draw connections (lines) and weight annotations\n    for (let l = 0; l < layers.length - 1; l++) {\n        for (let i = 0; i < layers[l]; i++) {\n            for (let j = 0; j < layers[l + 1]; j++) {\n                const x1 = (l + 0.5) * layerWidth;\n                const y1 = calculateY(l, i, layers[l]);\n                const x2 = (l + 1.5) * layerWidth - neuronRadius - squareSize - 2;\n                const y2 = calculateY(l + 1, j, layers[l + 1]);\n\n                // Draw line\n                svg.append(\"line\")\n                    .attr(\"x1\", x1)\n                    .attr(\"y1\", y1)\n                    .attr(\"x2\", x2)\n                    .attr(\"y2\", y2)\n                    .attr(\"stroke\", \"black\")\n                    .attr(\"stroke-width\", 1);\n\n                // Calculate position and rotation for weight annotation\n                const scaler = 0.8;\n                const annotationX = x1 + neuronRadius + (drawZSquares ? 10 : 20);\n                const slope = (y2 - y1) / (x2 - x1);\n                const angle = Math.atan(slope) * (180 / Math.PI);\n                const annotationY = y1 + slope * (annotationX - x1) - scaler*fontSize - 2;\n\n                const weightAnnotation = `w_{${j + 1}${i + 1}}^{(${l + 1})}`;\n                d3.select(`#${div_name}`).append(\"div\")\n                    .attr(\"class\", \"weight-annotation\")\n                    .style(\"left\", `${annotationX}px`)\n                    .style(\"top\", `${annotationY}px`)\n                    .style(\"font-size\", `${scaler*fontSize}px`)\n                    .style(\"transform\", `translateY(-50%) rotate(${angle}deg)`)\n                    .html(`\\\\(${weightAnnotation}\\\\)`);\n            }\n        }\n    }\n\n    // Draw neurons and neuron annotations\n    layers.forEach((numNeurons, layerIndex) => {\n        const x = (layerIndex + 0.5) * layerWidth;\n        const layerAnnotation = `Layer ${layerIndex}`;\n        svg.append(\"text\")\n            .attr(\"x\", x)\n            .attr(\"y\", 20) // Position at the top, you can adjust this value\n            .attr(\"text-anchor\", \"middle\")\n            .attr(\"font-family\", \"Arial\")\n            .attr(\"font-size\", \"16px\")\n            .text(layerAnnotation);\n        for (let i = 0; i < numNeurons; i++) {\n            const y = calculateY(layerIndex, i, numNeurons);\n\n            // Draw neuron\n            svg.append(\"circle\")\n                .attr(\"cx\", x)\n                .attr(\"cy\", y)\n                .attr(\"r\", neuronRadius)\n                .attr(\"fill\", \"steelblue\");\n\n            // Add neuron annotation\n            const neuronAnnotation = `x_{${i + 1}}^{(${layerIndex})}`;\n            d3.select(`#${div_name}`).append(\"div\")\n                .attr(\"class\", \"neuron-annotation\")\n                .style(\"left\", `${x}px`)\n                .style(\"top\", `${y}px`)\n                .style(\"font-size\", `${fontSize}px`)\n                .style('color', 'white')\n                .html(`\\\\(${neuronAnnotation}\\\\)`);\n\n            // Draw annotation for non-input neurons\n            if (layerIndex > 0 && drawZSquares) {\n                const rectX = x - neuronRadius - squareSize - 1;\n                const rectY = y - squareSize / 2;\n\n                const squareAnnotation = `z_{${i + 1}}^{(${layerIndex})}`;\n                d3.select(`#${div_name}`).append(\"div\")\n                    .attr(\"class\", \"annotation\")\n                    .style(\"left\", `${rectX}px`)\n                    .style(\"top\", `${rectY}px`)\n                    .style(\"width\", `${squareSize}px`)\n                    .style(\"height\", `${squareSize}px`)\n                    .style(\"line-height\", `${squareSize}px`)\n                    .style(\"font-size\", `${0.8*fontSize}px`)\n                    .html(`\\\\(${squareAnnotation}\\\\)`)\n                    .on(\"click\", () => propagateFromZSquare(layerIndex, i));;\n            }\n        }\n    });\n    \n    // Function to animate impulse\n    function animateImpulse(startX, startY, endX, endY, duration) {\n        const impulse = svg.append(\"circle\")\n            .attr(\"cx\", startX)\n            .attr(\"cy\", startY)\n            .attr(\"r\", 5)\n            .attr(\"fill\", \"red\");\n\n        impulse.transition()\n            .duration(duration)\n            .attr(\"cx\", endX)\n            .attr(\"cy\", endY)\n            .on(\"end\", () => impulse.remove());\n    }\n\n    // Function to propagate impulse\n    function propagateImpulse(layerIndex, neuronIndex) {\n        if (layerIndex < layers.length - 1) {\n            for (let j = 0; j < layers[layerIndex + 1]; j++) {\n                const startX = (layerIndex + 0.5) * layerWidth;\n                const startY = calculateY(layerIndex, neuronIndex, layers[layerIndex]);\n                const endX = (layerIndex + 1.5) * layerWidth - neuronRadius - squareSize - 2;\n                const endY = calculateY(layerIndex + 1, j, layers[layerIndex + 1]);\n\n                animateImpulse(startX, startY, endX, endY, 1200);\n\n                // Recursive call for next layer\n                setTimeout(() => propagateImpulse(layerIndex + 1, j), 1000);\n            }\n        }\n    }\n\n    // Function to propagate impulse from a Z square\n    function propagateFromZSquare(layerIndex, neuronIndex) {\n        if (layerIndex < layers.length - 1) {\n            for (let j = 0; j < layers[layerIndex + 1]; j++) {\n                const startX = (layerIndex + 0.5) * layerWidth;\n                const startY = calculateY(layerIndex, neuronIndex, layers[layerIndex]) + squareSize / 2;\n                const endX = (layerIndex + 1.5) * layerWidth - neuronRadius - squareSize - 2;\n                const endY = calculateY(layerIndex + 1, j, layers[layerIndex + 1]);\n\n                animateImpulse(startX, startY, endX, endY, 1200);\n            }\n        }\n    }\n\n    // Draw neurons and neuron annotations\n    layers.forEach((numNeurons, layerIndex) => {\n        for (let i = 0; i < numNeurons; i++) {\n            const x = (layerIndex + 0.5) * layerWidth;\n            const y = calculateY(layerIndex, i, numNeurons);\n\n            // Draw neuron\n            svg.append(\"circle\")\n                .attr(\"cx\", x)\n                .attr(\"cy\", y)\n                .attr(\"r\", neuronRadius)\n                .attr(\"fill\", \"steelblue\")\n                .attr(\"cursor\", \"pointer\")\n                .on(\"click\", () => propagateImpulse(layerIndex, i));\n\n        }\n    });\n    // Render MathJax\n    MathJax.typesetPromise();\n}\n"}]}
</script>
<script type="module">
if (window.location.protocol === "file:") { alert("The OJS runtime does not work with file:// URLs. Please use a web server to view this document."); }
window._ojs.paths.runtimeToDoc = "../../posts/fnn";
window._ojs.paths.runtimeToRoot = "../..";
window._ojs.paths.docToRoot = "../..";
window._ojs.selfContained = false;
window._ojs.runtime.interpretFromScriptTags();
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>